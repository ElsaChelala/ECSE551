id,body
1,"> Ezno started off as pet project to re-implementing the features of TSC in Rust

While reading that article, Rust definitely came to mind a few times. Looks interesting, although the example with any is really not the best."
2,">This differs from TypeScript which treats things like the return annotation as the source of truth, allowing this to be validly checked:

Typescript doesn't treat return annotations as the source of truth.

Example: [https://www.typescriptlang.org/play?#code/GYVwdgxgLglg9mABATwBQEMBciwgLYBGApgE4CU2AzlCTGAOaIDeAUAJAlFQglLosBfIA](https://www.typescriptlang.org/play?#code/GYVwdgxgLglg9mABATwBQEMBciwgLYBGApgE4CU2AzlCTGAOaIDeAUAJAlFQglLosBfIA)

You just don't understand what ""any"" is in Typescript. ""any"" means that it satisfies any type, all types. It can be string, it can be number, it can be violet sky. What you most likely wanted to do here should be written like this

    function y(a: unknown): string {
      // error, unknown is not a string
      return a;
    }
    
    // no errors
    function y(a: unknown): string {
      if (typeof a === 'string') {
        return a;
      }
      return '';
    }"
3,"Project Page (?): https://github.com/kaleidawave/posts

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/javascript) if you have any questions or concerns.*"
4,"Just so you know, there are very real compliance issues that companies face that may require their employees to be within certain countries or to spend no more than X days at any one time in restricted countries.  It's not because the company wants to be a PITA about it."
5,"Just went through the first few. Almost all require some office presence or will require the person to have a visa.

As a business owner, this makes sense. There’s a difference between posting a gig contract and hiring an employee. For an employee, you need to comply with legal / fiscal requirements. The posts reflect this reality.

I don’t think you really thought this through when you built it, or you would have realized this was going to be a major problem."
6,"There are two reasons:
1) not all companies are willing to do the legal filing for non-us employees. It’s extra work for them
2) not all remote work is async, so keeping teams in a limited set of time zones makes it way easier to schedule meetings"
7,"Something to be aware of here, it’s really super important to know is that working in a given location for more than a certain number of days can lead to tax implications not just for the employer but directly to the employee too. Assuming of course you want fully remote and not just remote in the US"
8,"I’m guessing that a site like this won’t be great for American workers to use? In the past it seemed like gig sites end up being a way for USA employers to get much cheaper workers from poor countries. I guess it’s good that you’re filling a need, though."
9,"Others are saying it already. But I’ll just add - as a remote employee who’s been remote for years, never expects to go back to work in an office again, and believes strongly in the importance of working remotely from where you are most comfortable - that any company that claims you can work from “anywhere” won’t offer that indefinitely and probably shouldn’t be offering it presently. I’d probably take it as a red flag that the company lacks a certain maturity or seriousness, we’re i to notice it in a job posting. 

There certainly are companies like that. I worked at InVision when it operated thusly. But it can’t last and, is frankly, delusional. There are very real fiscal implications for both the employee and the company.

Most companies that offer this sort of thing will actually want to hire you as a contractor in the country that you reside in. And most with a corporate counsel worth their salt (not to mention a board with any sense) will happily drop your ass if you try to “digital nomad” around the globe as a full-time employee. Not because they don’t believe in it (though let’s be honest they probably don’t) but because it exposes them to undue taxation if your travels come to light and they have to figure out how to pay some taxes in a country they don’t want to do business in."
10,"Writing tests can be difficult and time consuming, especially at first. But I believe tests are a key to long term stability of an application. They stop ourselves and others from inadvertently breaking things down the road, they document how an app is supposed to work and not work, and when you reach a certain level of proficiency can speed up your development cycle.

I was once a young programmer who built things without writing unit tests. When I joined a team that had high test coverage I was frustrated and did not understand, much like how your post reads.

I picked up the book TDD with Python and learned testing through the Django framework. Now I practice TDD and I am a better programmer for it. You may find someday you look back on this moment and feel proud of how far you came. Good luck, have fun, and don’t forget to write some tests!"
11,"nothing to do with your data, or anything, i havent even clicked through to look but what i find annoying is jobs advertised as remote to only find there's 4 days per week expected in the office etc. i looked for remote, for 100% remote, jeez. mind you i probably looked in the wrong place.

\*clicks your link\*

edit: the lazy loading is really a huge turn off for me. so wouldn't use your site on those grounds, sorry. in case you wonder why, it's because i've been burned thousands of times where i have to go through the list from the start again. etc. etc. but i guess you already know what i mean."
12,"Imo, if you spent the time applying for jobs instead of creating a web site scraper, *you would have the proper solution, which is employment.*

Also, is not a job in another country *remote?* I'm not sure *how much more remote you can get*. 

Now, you have a list of jobs you can't obtain.

Not sure why this topic keeps showing up in various subs. Likely some trend from some suggestion by someone slightly popular.

Also, if your criteria is to eliminate certain countries, how are the results *completely location independent?*

*What problem does this really solve?*"
13,"Github Link: [https://github.com/fvsionAI/fvsion](https://github.com/fvsionAI/fvsion)

  
I created a GUI/WebUI forStable Diffusion, using FastAPI as backend powered by Diffusers libary.

  
The motivation is to:

1. to learn by building things myself and mainly to separate the Backend (python) and Frontend (typescript)
2. to allow for easier tinkering for myself as well as others with more TS/JS background
3. to allow for api end point use with other development such as Photoshop/Blender or other plugin, just use Restful API to localhost:4242, might need to play around with CORS setting, currently opened to Vite port.

  
Please feel free to have a look at the code, clone and do your own fork. Released under MIT.  Appreciate any feedback.

  
I managed to create a standalone python server installer & electron exe working, but with relatively large file size (>2 GB ) which is beyond Github file limit. Anybody has suggestion of where to share this installer to?

  
Just happy to create for open source development, learn about FastAPi and manage to get it running. I kinda now understand why there is so many flavor of Linux. Sometimes you simply couldn't stop yourself to create something for the world even if only you yourself is going to use it.

p/s: hope this one fully follow the rules/guide for self promo. Do let me know if I do something wrong."
14,"Missing a bunch of common problems in the implementations:

When debouncing you often want to greedy-execute the function immediately and then discard/cancel the execution when the second call comes in. Eg: make an API request straight away, but cancel it and replace it with a new one on each debounced call.

When throttling you often want guaranteed eventual consistency - so that the last call is always executed eventually. Especially important with scroll handlers where you want the last scrolled to position to always be triggered eventually, even if some intermediate positions are skipped."
15,"`402.15s`

    const response = await fetch('https://registry.npmjs.com/-/v1/search?text=framework&size=250')
    const data = await response.json();
    const {objects} = data;
    const [first] = objects.sort((a,b) => b.package.date.localeCompare(a.package.date));
    const date = new Date(first.package.date);
    const since = Date.now() - date.getTime();
    console.log(`${(since/1000).toFixed(2)}s`);

Runs on node, but there's no CORS for browsers."
16,"Project Page (?): https://github.com/mauro-balades/secondssincelastjavascriptframework

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/javascript) if you have any questions or concerns.*"
17,"The only benefit I see to choosing another package manager is disk size. Something like pnpm where it symlinks all of your packages so they aren't duplicated across projects seems like a worthwhile benefit. But even still, I have never had so many projects active at once for that to be a concern. Sure I may have a few GB of redundant data on my computer, but when I still have hundreds of GB unused doesn't seem like a problem.

The install speed is mostly irrelevant. How often are you reinstalling everything? If you save even 5 minutes every few weeks when you update dependencies, is that meaningful? I don't think so.

I think I agree with you that the ""default"" tool (NPM) should be preferred simply for that. It's going to get the most support, most stable, most consistent, easiest to setup, etc. The downsides are real, but not big enough or impactful enough to bother using anything else in my opinion."
18,"I use yarn, cause it's a lot more configurable than npm, at leasy in my experience. When projects become large enough and you're dealing with monorepos, the extra features help.

I don't dig the PnP stuff tho, I always configure it to use the node modules linker"
19,"Whenever I see this kind of novel approaches, I wonder ""why is it MIT licensed?""

If it is so good, there is nothing stopping popular frameworks into copying the ideas. Won't that screw it over? 

Or is there anything preventing other frameworks to not copy its technique?"
20,"This seems ridiculous, but is it possible to just upload your CSS and JS to an external CDN and then reference that in the HTML file? Since you can reference Bootstrap on an external CDN, that should be possible for your own code as well."
21,"I think at first everyone loathes writing tests but after enough of your apps fail in production because things weren’t tested thoroughly enough you start to see the value in it.

What you’re going to want to do is hook up your tests to run automatically as part of ci/cd deployment so it’s functionally like a safety check that everything still works before anything can be deployed"
22,"I also agree with the webpack route.  However, you might find this post from last week interesting: https://www.reddit.com/r/javascript/comments/xdzmj5/a\_whole\_website\_in\_a\_single\_javascript\_file\_contd/"
23,"Is SSR needed or is it only CSR?

I agree with that other comment about hosting assets on CDN. Alpine.js is fine for simple apps.

React/Vue are good because of their ecosystem, but you might end up just fighting the frameworks. If HTML is generated by another system then these modern frameworks will not work well since they are supposed to control rendering. There are ways around this, but not worth the headache IMO.

BTW I remember working with HTML from the database back in 2007. It was a PL/SQL app and it was horrible. My condolences."
24,"I've done this in the past to avoid having to use the dumpster fire that is Eloqua.

I used this [webpack plugin](https://github.com/DustinJackson/html-webpack-inline-source-plugin) and it worked fine - you just need to host your images elsewhere"
25,"My project Greenwood might be able to help!  It supports web standards based development (e.g. not a meta framework) and has great support for Web Components.  As easy to start as an index.html file.

There is an optimization configuration option to inline your JS and CSS so you can still develop and load those files with script and link tags in the HTML (including from NPM), but have all those assets inlined when you build, and it will all come out in a single HTML file.
https://www.greenwoodjs.io/docs/configuration/#optimization

Can do most architectures; SPA / CSR, MPA, SSR."
26,"The pipeline operator currently sits at Stage 2 as an ECMAScript proposal. Many people (including me) believe the current proposal is a harmful addition to the language, and it would be better to _not_ have a pipeline operator than the current Hack proposal. For this reason, I created the linked ESLint plugin."
27,"I personally don't see the problem after spending too long reading the various threads on Github. 

Don't think I ever saw a single example comparing A to B which showcases how one is less readable than the other."
28,"/u/arendjr

I agree that the hack-style of `|>` is not what many of us hoped for. In fairness, there *are* some downsides of the F# version, which I also didn't like. So IMO it wasn't super clear which side should win. It was like a 51/49 thing in my mind. But I definitely would have liked some unary function compositions to be nicer, more like F# was pushing for.

I'm not sure if I would go so far as to say that hack-style `|>` has absolutely no place in my programs. But it's definitely not going to be, in its current form, something I use very much. I think I would perhaps rather have an eslint plugin that limits how `|>` is used, to avoid some of the absurd usages (such as the ones you point out), while still allowing it to be used in a few narrow cases.

I had my own issues with the limitations of `|>`. In particular, I [was pushing for the `pipe(..)` proposal](https://github.com/tc39/proposal-function-pipe-flow/issues/4#issuecomment-1179751775) as a way to support ""dynamic composition"" in a way that I didn't think `|>` could really handle. But unfortunately, `pipe(..)` was abandoned, because the same folks on TC39 pushing for hack-style `|>` decided that `pipe(..)` was not needed in JS. Super frustrating.

----

I [then proposed an extension](https://github.com/tc39/proposal-pipeline-operator/issues/275) to `|>` where the `...` operator could ""spread"" into a pipeline step, which I intended as a way to help `|>` serve that ""dynamic composition"" use-case.

Then I [subsequently pointed out](https://github.com/tc39/proposal-pipeline-operator/issues/275#issuecomment-1240263104) that it *could have* been a potential compromise between Hack and F#:

    // (1) F# style pipe composition:
    val |> something |> another(10) |> whatever
    
    // (2) Hack + F#:
    val |> ...[ something, another(10), whatever ]
    
    // (3) instead of:
    val |> something(^) |> another(10)(^) |> whatever(^)

The (2) was my proposed idea, where `...` could spread an array of unary functions into a pipeline. It's not as nice as F#, but it might have been close to a reasonable compromise.

Alas, as you can see, they still vehemently refuse to accept any contrary feedback that `|>` as it's currently designed is not sufficient. They're stuck on it only shipping as-designed and that's it. Very disappointing."
29,"I think everyone who start application development with Web eventually falls into this realization. Look how UI software is normally written and you'll realize that JS/Web pave their own path. React holds your hand, but almost all the demos are based around MVC structure which died years, maybe decades ago on desktop.

Angular tried to build this, but their change tracking system was really bad (literally performed a diff of all elements every ""tick""). They tried scoping later, and then model services, but it was kinda too late and the complexity has a buy-in cost. Then the whole Angular2+ being TS means you either had to jump ship or port all your existing code.

React was the better option because it computes a partial view and does a diff on that only when you update state. But those comparisons get costly as your complexity goes. But people will stick with React because JSX. Even though you don't need React to use JSX (it's literally just two functions), the integration is clean.

Svelte is much closer to standard UI design with imperative calls for drawing, but add an abstraction layer so you can use declarative syntax (like React), that devs don't see or experience. The trick is they compile declarative down to imperative. They ditch the [VDOM](https://svelte.dev/blog/virtual-dom-is-pure-overhead) entirely.

Still, they all follow around the design of your data exists twice (or even three times): On the page, and on an internal, component-level state (three if you try to use a model service). If you can move away from MVC and try MVVM or MVP and you can ditch (almost) all the state comparisons. But I wouldn't worry about moving unless the complexity starts causing a drain.

I'm suspecting* the next big language will probably adopt `WeakRef` and native `Web Components`. IE11 has really hamstrung us for years. `lit` looks like a good one, but doesn't really tackle the replicated state issue. They all bind to self, instead of binding directly to the model. Getting people to put all their data outside the component was really an Angular thing, but lines of code need to get it setup was not very inviting."
30,"Article makes some good points, but there's a few things that are questionable - like he says that context was a replacement for redux. Uhhh no. There's some great state management solutions out there that are not considered at all here. Managing useEffect dependencies seems pretty simple. The useMemo useCallback stuff is valid but it might be fixed in a future update.

I've been around for a while, so I know that React will be replaced sooner or later by something that makes different tradeoffs and will be hailed and the greatest thing ever, and then five years later we will do it all again. Pressure is just mounting for that change.

Forms have always been pretty annoying in React though."
31,"I really agree with this, 1000%. Glad to see it's not just me. Been starting to look into svelte just to see if the grass is greener. So far it's a breath of fresh air. 

But if course the intellisense/ts validation doesn't work in templates, since it's not really Javascript but a templating language. But maybe it's worth it still"
32,"https://www.amazon.de/JavaScript-Parts-Working-Shallow-Grain/dp/0596517742/ref=mp_s_a_1_2?crid=XAGSZ55URTJI&keywords=Douglas+Crockford&qid=1665946105&qu=eyJxc2MiOiIxLjk1IiwicXNhIjoiMS41MCIsInFzcCI6IjEuMTUifQ%3D%3D&sprefix=douglas+crockford%2Caps%2C306&sr=8-2 I believe, this one may still be relevant."
33,"Even the ecosystem is questionable.

MUI for example is bloated and badly designed, they are still talking about how the styling and theming should be done and there are like 7 different ways of styling. They are not even utilizing css variables, nobody even though about that lol, but the important thing is that they have 2 styling engines.

I also opened dev tools and i got and infinite loop in mui table for example even though i copied their example from docs.

They are always these annoyances in the ecosystem. Like react-hook-form for example.

Yes, it's easy if you have email and pwd but as soon as i want  to have field array with number-format and get unformatted value properly in state, you have to go around bunch of these hoops and hacks to make it work. 

tell me 5 packages that you **really** need.

ok react-query is nice but will be also available fully in other frameworks (it works already for svelte).

drag n drop exists in svelte, wysiwyg editor also is framework agnostic and works for svelte.

is there really something that people can't ""live"" without"
34,"I get the frustration with libraries. All of them have their limitations. React cannot be responsible for 3rd party libraries. All of them are useful until a certain level of complexity, and then they become a hinderance rather than a help.  Redux can be great until it isn't. It's a double edged sword. I think the option for controlled versus uncontrolled inputs makes a lot of sense.

However the refs thing, that's completely on the author. If you are using that many refs you are using the tools that react provides in the wrong way. It might be feasible but that doesn't mean that's what you should be doing. The documentation on the react website is pretty clear to keep refs to a minimum.

>from the React documentation ([https://reactjs.org/docs/refs-and-the-dom.htmlhttps://reactj](https://reactjs.org/docs/refs-and-the-dom.htmlhttps://reactj)...There are a few good use cases for refs:Managing focus, text selection, or media playback.Triggering imperative animations.Integrating with third-party DOM libraries.Avoid using refs for anything that can be done declaratively.Don’t Overuse Refs:Your first inclination may be to use refs to “make things happen” in your app. If this is the case, take a moment and think more critically about where state should be owned in the component hierarchy. Often, it becomes clear that the proper place to “own” that state is at a higher level in the hierarchy. See the Lifting State Up guide for examples of this.""

Raising the state might be the most important think you need to know about React. Form inputs with states that also have to communicate with other inputs is a design flaw."
35,"Could Svelte be the next game-changer?  I've only used Angular, React, and Vue so idk what it's *really* about.  Writing less code and delegating things like performance optimizations to the behind the scenes magic of the framework is definitely ideal, but is there a some downside to using Svelte over React(besides what I assume to be a much smaller ecosystem)."
36,"Just use vue.


- Truly reactive (unlike React lol). you don't have to call functions to update the state.

- The v-model is fucking amazing and it can be applied with any data structure you want with any custom components as well.

- Intuitive and no unnecessary boilerplate state management with Pinia

- Easy af router with Vue routers and just install vite-plugin-pages and vite-plugin-layout to implement automatic route generation and layout support

- Lighter bundle size, more performant

- If you are really a JSX fanboy, you can use that as well (or good old render functions)

- With Volar, amazing typescript IntelliSense

- VueUse lib with most of the utilities you'll ever want to write with amazing typescript support.

- write everything for a single component in that file, if you want. if you want to separate, you can do that as well.

- never encounter any infinite rerender errors.

- don't have to manually state the dependency array just to get a computed value. (Vue is smart enough to recognize them and do that on its own)

- with script setup, write js or ts as normal without having to deal with options API.


the list goes on, and if you dislike Vue, you can try Svelte, which has most of these features and some things are arguably better than vue, but the community is not mature compared to vue and much less mature than React.
Some web developers are dense af, they never want to try another tool if what they are using feels good enough for them. Most react folks use outdated arguments from early vue2 days to shit on Vue. Developing with React despite all of its problems that most other frameworks have successfully solved by now doesn't make anyone cool."
37,"Agree with this articles points about all the deficiencies with hooks and context. Felt that for a long time. Still, I really want for react to succeed, because the “just javascript” aspect of it makes it so much more powerful to me than any templating engine. I find it much more intuitive to write real code that generates layouts than be shoehorned into a templating system. The frustrating part is that i feel all the grievances he listed here are fixable - but who knows if they will be"
38,"Odd one out. Am I the only one who wants to go back to PHP / Laravel. Building anything with JS/TS seems unnecessarily complicated, TS is great for killing bugs but In slowly going back to PHP wherever I can"
39,"Using Vue3/Nuxt3 has been incredible. It's so refreshing to feel like I can just code in JavaScript and build in HTML. React feels many times to be completely over-engineered.  Also really not a big fan of changing traditional conventions of JS and HTML.  no seperarion of concerns from presentation and data layer is just messy and suffers from low readability. Vue3 has been great, I've heard svelte is making strides too. I'm over react."
40,"Regarding forms, Nothing beats angular Reactive forms imo. It's typed,and it has terrific validation options.

TypeScript is also super in Angular 

I find react more developer friendly in general, but the aforementioned topics, I think angular kills it"
41,"They Complain about React then shows off poorly engineered React to reinforce their arguments.

One really simple example. If your Inspector is returning null if isVisible is false, why even render the component? Here component, please load and do nothing, thanks."
42,"One thing that sucks about React is how react router v1 and v2 are so different that v2 could be renamed Foobar router and nobody would notice.

Nobody seems to care that there are real projects using their libraries and that we can't all be rewriting our whole app at every major release.

Another thing is how tiny companies are using React, a framework designed to solve problems at Facebook scale, to solve their tiny little problems that will never need scaling."
43,"Colocated CSS modules are what I've been doing for the last few years.

If anything is driven by a variable, that can either go into a style object or update a CSS variable.

By this articles own arguments, I still see no reason to complicate anything any further. This still has major ""shiny new toy"" vibes."
44,"If you want to become a MERN developer then focus on learning Frameworks instead.

\- Learn React

\- Learn typescript, because people rarely write pure js.

\- Learn about the Node.js ecosystem and about the tools that everyone is using nowadays (eslint, webpack/vite, jest,...) and how to set them up for your project

&#x200B;

Advanced Javascript usually contains Browser APIs like proxys and stuff that are used by frameworks and that you as a dev won't touch, unless you're writing your own framework."
45,"I don’t understand the author’s issue with managing useEffect (and useCallback/useMemo) dependencies. I prefer having control over that behavior, just because a value is used in the callback doesn’t mean it needs to be in the dependency list."
46,"Personally, I've had no problems with `.env` files, especially when it's a universal concept that other projects in other languages also use. As a team lead who handles different backends, I find `.env` files easy to manage. Nothing wrong with some thought experiments though.

Very clickbaity title, though."
47,"I feel like the author has a solution for a problem that should not exist. 

In my >7 years dev experience .env files are used for development only. For proper deployed environments, CI/CD is used and then environment variables are handled via system that handles your CI/CD pipelines ( Jenkins, Github, TeamCity etc. ). So it's either passed to docker containers or set in containers / servers where the app runs. 

So to have the same functionality locally, create env file with variables you need locally. 

You don't need another external system to store .env variables if you don't have very complex, multi-tenant system that uses a lot of these variables."
48,"I like how concise your examples are.  But I don't know that you are showing reactivity, really, except for the `computed` example.  The others are just DOM elements reacting to DOM events, not really to a change in reactive data.

`reactive` doesn't really do anything different than `ref` would do in your example.  Same with `watch` and `watchEffect`.  It would be cool to see `toRef` used also since that's a little bit under-represented in the official docs."
49,"Thanks for sharing your open source project, but it looks like you haven't specified a license.

> When you make a creative work (which includes code), the work is under exclusive copyright by default. Unless you include a license that specifies otherwise, nobody else can use, copy, distribute, or modify your work without being at risk of take-downs, shake-downs, or litigation. Once the work has other contributors (each a copyright holder), “nobody” starts including you.

[choosealicense.com](https://choosealicense.com/) is a great resource to learn about open source software licensing."
50,"Oh nice. I have this public api i am working with that i’d love to see if this can handle it. I think the openapi definition is like 2-3mb (code that is generated from the spec is over 100k lines)

Thanks!"
51,"Your coined phrases made me smile. :)  At a former gig, we referred to the front of the front end engineers as ""UX Engineers.""  Responsibilities were almost spot on, but typically they didn't do much of the actual js dom manipulation.  A lot of figma designs, css (scss), and templating though.   I'm sure the dividing line varies by shop.  I'd imagine you'd find more of the front of the front end engineers at larger (30+ engineer) shops - or are you seeing that in smaller teams as well these days?"
52,Good thing if You want to slow down the development process. Doing classical frontend isn't that hard for it to be split in two. Learn styling or learn doing logic if you suck at one thing. In modern frontend component architecture You cannot separate crafting page structure from doing logic. Back-of-the-frontend fellas would get crushed by need to cut jumbled HTML into logical components.
53,"One quirk that runs parallel to this:   
You are allowed to spread falsey/undefined values into an object  
```
{    
    ...undefined, // this is ok
    ... condition && {some: ""values""} // this is too
}
```
but to do a similar operation in an array, you'd need to do:
```
[
    ...condition ? [values, to, spread] : []
]
```"
54,"I have mixed thoughts on it. I don't like the syntax and find it very unintuitive, but it might provide a much shorter way of creating complex objects with lots of conditional stuff.

So, would this work?

```
const combined = {
  ...cond1 && obj1,
  ...cond2 && obj2,
};
```

I'd also want to look more into other similar uses of logical operators in conjunction with the spread operator. Things like `||` and `??`.

Really though... Using the spread operator on a boolean but having it operate on the object just seems really wrong."
55,Ey JS gurus I have a weather application that I'm trying to develop in JavaScript.html.css. I'm new on js been doing Java now I'm moving to Angular since I'm now on spring Boot. Can anyone contribute to the project?
56,"This relies on the fact that spreading `false` won't result in any copied properties, because when it gets autoboxed it doesn't have any enumerable own properties. That's some pretty specific knowledge to need to know, which is why I would avoid doing this."
57,"A nice short hand to know, I believe using the ternary operator is best for readability and consistency when dealing with spreading both objects and arrays.

```js
{
 ...values,
 ...(condition ? { title } : {}),
 ...(condition ? { description } : {})
}
```

```js
[
 ...values,
 ...(condition ? [ title ] : []),
 ...(condition ? [ description ] : [])
]
```"
58,"Oh I always did something like

```
{
  ...otherProps,
  ...(isActive ? theObject : {}),
}
```

Works for arrays too. You can use the same idea to conditionally add properties inline

```
{
  ...otherProps,
  ...(isActive ? { relevancy: 100 } : {}),
}
```

it falls in line with the ideology of being explicit, like for example force return void on handlers for function calls with the `void` operator.

```
<SomeComponent onClick={() => void onClickHandler()} />
```

The reson for this is that then you don't need to worry about what the function returns and avoid hard to track bugs (ie, if the function accidentally returns a value, which depending on the value can lead to unwanted behavior).

While sidetracking a little, the `void` operator is pretty cool and underratted even for 2022, especially useful for unnesting overcomplicated conditionals. Take this piece of code for example:

```
function doSomething(active, focused, id) {
  if (active) {
    if (focused) {
       focusHandler(id);
    } else {
       otherHandler(id);
    }
  } else {
    Logger.error('Needs to be active');
  }
}
```

Can be simplified into a much cleaner code like that:


```
function doSomething(active, focused, id) {
  if (!active) return void Logger.error('Needs to be active');
  if (!focused) return void otherHandler(id);
  focusHandler(id);
}
```

cool right? More on that here: https://www.youtube.com/watch?v=EumXak7TyQ0"
59,"Honestly, I just use immer for logic like this now. It keeps everything immutable and the conditional logic easy to follow. Otherwise just mutating an object can be fine a lot of times if it's confined to a small enough scope."
60,"If you would like this roundup sent to your reddit inbox every week send me a message with the subject ['javascript'](https://www.reddit.com/message/compose?to=subredditsummarybot&subject=javascript&message=x). Or if you want a daily roundup, use the subject ['javascript daily'](https://www.reddit.com/message/compose?to=subredditsummarybot&subject=javascript%20daily&message=x). Or send me a chat with either javascript or javascript daily.

####Please let me know if you have suggestions to make this roundup better for /r/javascript or if there are other subreddits that you think I should post in. I can search for posts based off keywords in the title, URL and flair. And I can also find the top comments overall or in specific threads."
61,"https://github.com/samchon/typescript-json#runtime-validators

Until yesterday, `typescript-json` only had supported type validators which can't prohibit superfluous properties that are not listed on its type (`T`). However, from now on, you can prohibit such superfluous properties by below equivalent validator functions.

  - `equals()`: returns boolean
  - `assertEquals()`: throws exception
  - `validateEquals()`: archive all type errors

```typescript
import TSON from ""typescript-json"";

interface IPerson {
    name: string;
    age: number;
}

const person = {
    name: ""Jeongho Nam"",
    age: 34,
    account: ""samchon"", // superfluous property
};

TSON.is<IPerson>(person); // -> true, allow superfluous property
TSON.equals<IPerson>(person); // -> false, do not allow
```

Also, below is newly revised entire functions supported by `typescript-json`.

```typescript
import TSON from ""typescript-json"";

//----
// RUNTIME VALIDATORS
//----
// ALLOW SUPERFLUOUS PROPERTIES
TSON.assertType<T>(input); // throws exception
TSON.is<T>(input); // returns boolean value
TSON.validate<T>(input); // archives all errors

// DO NOT ALLOW SUPERFLUOUS PROPERTIES
TSON.equals<T>(input); // returns boolean value
TSON.assertEquals<T>(input); // throws exception
TSON.validateEquals<T>(input); // archives all errors

//----
// APPENDIX FUNCTIONS
//----
TSON.stringify<T>(input); // 5x faster JSON.stringify()
TSON.application<[T, U, V], ""swagger"">(); // JSON schema application generator
TSON.create<T>(input); // 2x faster object creator (only one-time construction)
```"
62,"It was me. I coined the term “spread operator”.

It was a lonely night in 1993. I was angry at the world and I needed an outlet. I thought that by introducing an incorrect term to the javascript community, I could spread some of my pain to save myself at the expense of others.

I had no idea the impact I was going to cause on the javascript community and the world at large. I am horrified to witness you suffering from my mistake all these years later. I am truly sorry. I would do anything to go back and fix things.

Is there any way you could forgive me?"
63,"I'm not sure why you think someone here is going to have an answer to your question. Seriously, how would any of us know who the very first person was to say operator.

In addition, just because the spec calls it an element doesn't mean that it is not behaving as an operator. In fact, the spread element _is_ an operator for both spread and rest expressions.

In other words, the spec may call it an element because it has those two varying contexts; operator behaviors, if you will"
64,"“Someone used the wrong word on StackOverflow and now I demand an explanation”

People talk about the “spread operator” to communicate, not to follow the ECMAscript docs. You clearly weren’t confused by the term term, so the term serves its purpose.

I’d imagine this term came from hundreds of independent sources around when ES6 came out, not that that effects anyone.

Frankly, I don’t see why the distinction matters unless your parsing JS syntax. For anyone writing a parser there is the ECMAscript documentation written to your exacting standards."
65,"Is there a prize bounty, I might do the research?

Why do you need that information?

In my native language we often times use “zeroth” element for the first element in array. This is technically incorrect so a friend of my had a discussion with a linguist about it. Seems similar to your problem.

The linguist guy basically said that the language is a living thing. It’s not created by the few and then accepted by the others, it’s exactly the other way around."
66,Why not generating the SDK based on the swagger yaml? This already exists and still gives you the advantage to work contract first so backend and frontend can work in parallel when implementing a feature.
67,"So far the earliest usage I have located is https://www.oneminutecoder.com/

> Jan 28, 2009 — An Example Of JavaScript ...spread operator ...

Following the TC39 proposal we still find that language https://github.com/tc39/proposal-object-rest-spread/blob/7d6d8a5de6ca75c2370cdef764b935fc864e52de/Rest.md.

This dates to 2014 https://github.com/tc39/proposal-object-rest-spread."
68,"Very clear and to the point, I enjoyed it! Some additional use cases might be good. For example, can you use matching strings longer than one character? Showing how to use “position” with “includes”, etc. Nice work!"
69,"It looks like emscripten has some support for [openGL](https://emscripten.org/docs/porting/multimedia_and_graphics/OpenGL-support.html). 

It seems like it might help with performance. Have you tried to use that for pcl.js? 

Also, you forked the PCL project to integrate it with pcl.js, which means you'll either have to keep your fork updated or fall out of sync.  
Would it have been possible to keep your changes in the pcl.js repo and build against the vanilla PCL repo?"
70,"Been writing a bit of vanilla JS lately and I wholeheartedly agree with this article that it's definitely a viable choice. Browsers, and bundlers, have come a long way. 

Enjoyed the read, and learned a few new things as well."
71,"I prefer to export functions from a module.  Static state can be modelled using mutable variables in the module, but it's pretty rare that you'd do that.

There's no need to force someone to instantiate a class, or to provide further isolation by encapsulating such methods statically inside of a class inside of your module."
72,"Unless you are using variable encapsulation/have state related variables in your class, you can just make an object with your functions as keys, I.e: Record<string, () => Promise<unknown>> and don’t really need classes."
73,"It really depends on the rest of the class. But, given that this is probably an authenticated request, I'd side with it being an instance (normal) method since I'd expect credentials to be something passed to the constructor."
74,"Usually static methods are considered an anti-pattern (similar to singleton). If you have any type of IoC / Dependency Injection framework, I'd suggest using a normal method & then referencing it from the instance of the class instantiated from the Dependency Injection.

&#x200B;

DependencyInjection is more work, but it creates a better architecture for a large application (reduces coupling). If you're just doing a quick/dirty throw-away project, I'd use the Static."
75,"Or do what everyone else with a brain does.

Use NextJS because it adds the most extensibility and control over rendering to React, but only use the parts you absolutely need.

Newbies tend to think they have to use every part of it and run to NextJS documentation to solve every problem.

I think this article plays on the naivete that a lot developers have about what SEO is, and why we have so many options around rendering. It is NOT just about static HTML.

I think I do not care about this point on AX. Yes, Next is NOT NOT NOT a fullstack framework. I've been yelling that from the rooftops every time some noob recommends using it the same way they use Rails. NextJS calls itself a **React Framework**. It gives SSR capabilities to React and React alone. If you're using Next to build real APIs, that's on you. Not on them.

"" I often feel like Next is designed to be a framework specifically meant to run on edge or serverless platforms."" .....No shit dude. The vercel platform exclusively hosts **serverless frontends**. It will not host Node servers. Have you read any of their marketing material? Even the homepage? Serverless frontends that may still interact with decoupled APIs. I really don't get the issue here. The point is to run APPLICATIONS on the Edge, not cache HTML on CDNs.

No one should be using it to build entire backends. But it sure is nice to have app level scoped server routes now and then. If not just to build a sitemap and RSS feed. Or to make initial data calls on the server like with Apollo Client and TanStack Query. Those are HUGE boosts to speed.

This entire thing comes across as a total misunderstanding of modern development. We decouple frontends and backends for good reason. That does not remove the problem of a frontend needing specific service. A decoupled backend should not be building HTML or sitemaps, it shouldn't be preparing data for a specific state manager to rehydrate from, it shouldn't have to handle third party auth, etc. This is why we need server side tools in frontend frameworks.

Just because Next provides API routes, doesn't mean they've implicitly declared it a fullstack framework.. And if all you're thinking about is static generation and this naive view of ""SEO"", you've never used it right.

This whole thing is so lost and confused. It got to the point where you were just rambling about nothing. I often enjoy a good long rant, but dude, stfu."
76,"Sometimes I think the JavaScript community, way back when it innocently decided to eschew classic server-side MPAs, and strike out on it's own path to the ultimate architecture, and clung to SPAs and DOM rendering on the server, to create SSR and meta-frameworks, with bizarre tier-fluid architectures, actually ended up cursed with more ongoing learning struggles than was ever worth the bargain of not having to learn classic server-side languages and ecosystems in the first place."
77,"Ever tried ng-swagger-gen or ng-openapi-gen? BE doesn't have to do much work, just add annotations that are mostly done anyway. With some BE frameworks all the work is done automatically as well.

FE just needs a configuration file and running the generator and that's it. You get all models and services. I don't see the need for your approach in the majority of cases, it is in fact more work"
78,"Tuples (i.e. arrays) are less optimized than objects.

Compare the following two functions:

    function getSizesObject() {
      return { width: 100, height: 50 }
    }
    
    function getSizesTuple() {
      return [ 100, 50 ]
    }

In an ideal world, the two functions should cost approximately the same. However, when you destructure the return value immediately...

    const { width, height } = getSizesObject()

...the VM can infer that the object itself is not needed, just the values. It can then elide allocating the object and store the individual values on the stack. This can reduce GC pressure and improve performance in tight loops.

Unfortunately, no popular JavaScript engine optimizes the tuple version as well as the object version.

    // This is slower
    const [ width, height ] = getSizesTuple()

Source: Ran benchmarks myself recently.

----

Also, tuples require you to remember the order and meaning of each field (array item), whereas object properties have names and can be accessed without caring about order. IMO there's very little use for tuples, except for compliance with the built-in methods. (E.g. `Map.prototype.entries()`, `Array.prototype.entries()`)"
79,"While most of this is good, I have some counterpoints to your blanket statements:

> If you see it was released months or even years ago, search for an alternative and don’t waste time evaluating it

It really depends on what it is and what it does. If the module does some physics calculations then it can be years old and it doesn't matter because it shouldn't ever need to be updated. Many libraries are ""done"" and that is okay. As long as it still works, it doesn't need constant updates. There's many types of libraries where constant updates are a red flag since they would be creating churn or increasing bundle size for the sake of it rather than trying to solve an actual problem.

> Companies behind the module

Not all companies are created equal. Some companies let their employees publish open source packages under their namespace which are effectively instant-abandonware since no one at the company is paid to maintain (or even know about it) and if that creator ever leaves the company they now cannot touch the thing they created since they don't have any rights to it, the company does. So just because it was created by a big company you've heard of, that shouldn't be a factor. A lot of open source collectives and individuals are far better maintainers than large companies.

It's also important to note that companies will invest heavily in their library as long as it stays useful to them. For example, if Meta stops using React internally, you bet that it'll quickly die since there'll no longer be investment and as much care as everyone moves over to whatever the next thing is."
80,"Hey u/olvrng it's cool that you have a blog and thanks for sharing it. 

However I think it's more useful if you try to find topics that don't already have about one billion identical posts written by other people. 

https://www.google.com/search?as_q=new+features+in+es2022

What does your blog post add to these existing posts? The answer is _nothing_, so why did you write it?  

Try to find something _new_, or at least a _new angle_ for an old topic. Make your content useful to people. Your blog is a service. If you just pick an obvious and easy topic like this, which has already be done by a thousand people, you're just adding noise. The only person being served by this is you."
81,"Medium is already the worst platform for blogging but posting code snippets as regular comment blocks, without indentation (or syntax highlighting/monospaced font) makes it even worse.

Presentation matters. How you could put effort in a post and than present it like that is just mind boggling.

Of course it’s self promotion. That answers the question of why someone would want to share this in Reddit."
82,"It feels a little bit wrong to just “throw away” the preposition arguments as in `add(15, and, 10)`. Especially since this would allow for using “wrong” prepositions like in `add(5, by, 10)`.

I feel like you are looking for enforced named parameters like they are widely used in Python:

    def add(n, *, and):
        return n + and
    
    add(5, and=10)  # correct
    add(5, 10)  # exception
    add(5, by=10)  # exception

They have a preposition vibe. JavaScript does not have named parameters, but they can be emulated with object parameters:

    function add(n, {and}) {
        return n + and
    }
    
    add(5, {and: 10}) // correct
    add(5, 10) // fails
    add(5, {by: 10}) // fails

Note that I don’t endorse actually writing functions like this. I just feel like this would be a cleaner way to reach the same goal?"
83,"Thanks for sharing your open source project, but it looks like you haven't specified a license.

> When you make a creative work (which includes code), the work is under exclusive copyright by default. Unless you include a license that specifies otherwise, nobody else can use, copy, distribute, or modify your work without being at risk of take-downs, shake-downs, or litigation. Once the work has other contributors (each a copyright holder), “nobody” starts including you.

[choosealicense.com](https://choosealicense.com/) is a great resource to learn about open source software licensing."
84,"generator functions are most often used when you need to do some operations on a large number of values which would take up too much memory if stored in an array. 

It's similar to how lazy evaluation in functional languages like Haskell works."
85,"Let's say you're prototyping something and you need a quick and dirty function for making in-memory sequential IDs just to test an idea out, so you write:

    const generateID = (() => {
      let id = 0;
      return () => id++;
    })();

    const id1 = generateID(); // 0
    const id2 = generateID(); // 1

That uses a closure to encapsulate that `id` variable and keep it safe from outside modification, but this is more idiomatically expressed using a generator function (with the advantage over the previous solution that it can be re-used):

    function * idGenerator() {
      let id = 0;
      while (true) {
        yield id++;
      }
    }

    const idIterator = idGenerator();

    const id1 = idIterator.next().value;
    const id2 = idIterator.next().value;

Another use case is for generating Python-like ranges:

    function *range(start, end, step = 1) {
      let current = start;
      while (true) {
        yield current;
        current += step;
        if (current > end) break;
      }
    }

    const nums = Array.from(range(2, 15));

Perhaps you're writing a Discord bot and you want a function to extract all the @mentions in a message:

    const mentionRegex = new RegExp(/@(\w+)/, ""g"");
   
    function *getMentions(messageString) {
      let match = null;
      do {
        match = mentionRegex.exec(messageString);
        if (match) yield match;
      } while (match);
    }

    const string = ""the question asked by @leonheartx1988 was answerd by @thegaw and @ggcadc"";

    for (const mention of getMentions(string)) {
      console.log(mention);
    }

Which should output:
    [""@leonheartx1988"", ""leonheartx1988""]
    [""@thegaw"", ""thegaw""]
    [""@ggcadc"", ""ggcadc""]

You've probably noticed a pattern here - all of these involve some kind of necessity for lazy evaluation. A simpler example of where this is useful are collection methods like `filter` and `map`. Consider the following example, where you have a `VDOM` node instance (as you might find in a framework like React) and you want to extract all the `onClick`, `onMouseenter` event names defined on it:

    const eventNames = Object.keys(vdomNode)
      .filter(key => key.startsWith(""on""))
      .map(key => key.substring(2).toLowerCase());

In that snippet, `.filter` returns an array, and `.map` returns another array. There is an intermediary array between the input and the output, which can be avoided by using lazy evaluation:

    function *filter(iterator, predicate) {
      for (const value of iterator) {
        if (predicate(value)) yield value;
      }
    }

    function *map(iterator, transform) {
      for (const value of iterator) {
        yield transform(value);
      }
    }

    const vdomKeys = Object.keys(vdomNode);
    const vdomEvents = filter(vdomKeys, key => key.startsWith(""on""));
    const vdomEventNames = map(vdomEvents, key => key.substring(2).toLowerCase());

This returns a lazy iterator, not an array. Depending on the size of the data, this potenti"
86,"Generators are a powerful (if often misunderstood) feature that can be molded to operate in a variety of different ways. We typically call that ""metaprogramming"".

The design of generators being such a low-level primitive, where the code in the generator is driven/controlled by the separate iterator, allows a nice abstraction (separation of concerns), where the ""driver"" that controls the iterator has almost complete control to interpret yielded values in arbitrary ways, as well as the values that are sent back in with each iterator `next(..)` call, but all *that* driving logic is neatly hidden away from the code you write inside the generator.

One important point: it should be noted that rarely are generators *the only way* to accomplish something. Pretty much everything I will point out below, *could* be kludged together without generators. Indeed, programmers have done this sort of stuff for decades without them. But generators are so powerful because they make tackling such tasks *much more* reasonable and straightforward in code.

----

I've written several libraries that build on top of the metaprogrammability of generators. In these libraries, the user of the library writes and provides a generator with a certain pattern or style of their own code, and under the covers, the library drives that generator code with extra functionality pushed on top of it.

One such example is implicitly applying a `Promise.race(..)` to any `await pr` style statement. [The CAF library](https://github.com/getify/CAF) does this, using generators to emulate the `async..await` style of code, but where there's automatic subscription to cancelation tokens so that any of your async code is cancelable externally.

Another example [is the Monio library](https://github.com/getify/monio) which allows you to do do-expression style monad compositions in a familiar'ish imperative form (again, somewhat like `async..await` style), where under the covers the `yield`ed values are monadically chained together.

I've written several other libraries that use generators similarly. And as others have mentioned or linked to, there are a number of more well-known libraries, such as ""Redux-Saga"" and ""co"", that did the same.

----

Now, if we were not just talking about generators used for metaprogramming purposes to implement certain design patterns, the other main purpose of generators is to provide a very nice mechanism for expressing ""lazy iteration"".

If you have a data set (either in some source like a database or file, or that you will programmatically generate) that cannot reasonably be put entirely into a single data structure (array, etc) all at once, you can construct generators (as iterators) that will step through the data set one item at a time, thereby skipping needing to have all the data present at the same time.

Say for example you wanted to take an input string of typed in characters (perhaps of a length greater than say 15) and step through all possible permutation"
87,"You can use (async) generator functions to implement a simple faced for an API with pagination. Let's assume that we have an API which returns the following JSON:

```
interface ItemSet {
   /* Batch of items. */
   items: Item[];
   /* Link to the next batch of items. */
   next?: string;
}
```

Than we can write an async generator function like this

```
async function* items(query) {
   const url = queryToUrl(query);

   let result = await fetch(query);

   while (result) {
       // loop and yield each item
       for (const item of result.items) {
           yield item;
       }

       // or yield without explizit loop
       yield* result.items

       if (result.next) {
           result = await fetch(result.next);
       } else {
           result = null;
       }
   }
}
```

which allows us to query items in clear and readable way:

```
const query = { sort: '...', filter: '...'  };
for await (const value of items(query)) {
    // do something
}
```

The whole beauty about this is, that we can use `break` and `continue`, while processing our results:

```
const query = { sort: '...', filter: '...'  };
for await (const value of items(query)) {
    // abort loop
    if (breakCondition) {
        break;
    }

    // process next item
    if (continueCondition) {
        continue;
    }
}
```"
88,"YES please!  I couldn't find any useful info on this.
1) How to run and debug an extension
2) How to edit, save, and reapply an extension

Not even making one from scratch, but how to make an edit to an existing extension you're using so you can submit a pull request."
89,"I've wrote a couple of them, but can't really recall what for :D BUT async generators on the other hand are much more useful - at least i've used them more - to eg.: chunk load data from a remote server and it can be consumed with a for await loop also the same way you can process a stream or a mongodb cursor.

You can ofc solve the same thing with other techniques, it's just yet another fancy tool on the belt."
90,"It's useful for paginating through an API, see the [AWS JavaScript SDK](https://aws.amazon.com/blogs/developer/pagination-using-async-iterators-in-modular-aws-sdk-for-javascript/) for example.

If you're an application developer, you likely will not **write** generators but you may _consume_ them. It's more of a feature for library authors. It allows a library to offer a more intuitive API to reduce complexity for users of the library."
91,"The main way I’ve used them is for defining tasks in [Ember Concurrency](http://ember-concurrency.com/docs/introduction/). There are some other libraries that are similar as well for working in other ecosystems. 

These libraries use it a lot like a version of async/await that you (or the library) can choose to stop running at any `yield` point. This can be pretty useful when writing up a sequence of async functions; you might decide that you don’t need the result anymore (like if the user changes pages) and can use the ability to stop pulling values from the generator to opt out of work that would be thrown away anyway."
92,"While I've only used them in redux-saga in JS, the general use case for this kind of structure is when you've got a long list of things where you might not need to access them all.

Imagine for example that you've got a list of ten million calculated elements but you're going to stop processing them when you find the one you're looking for.

If you find that element early on, you can save a lot of resources by just stopping, which generators allow you to do."
93,"Lazy evaluation of sequences is more useful than you might think. It lets you separate the concerns of generating sequences from other processing. Look for big loops with a lot of exit conditions, filtering, or manipulation inside. Those can often be decomposed really nicely with constructs like generator functions."
94,"I've used them a few different times but they are definitely very niche. One case that I think is the most ""by-the-book"" is a function that performs a series of calculations and you may or may not need the intermediate products of those calculations. Using a generator you can let the implementing code just step through the products and use the ones it wants. This is similar to the standard example of stepping through an otherwise infinite loop. Honestly most of the time I end up making these as discreet functions for each step, it's not as DRY but it helps with readability a lot.

The most recent example was a situation where I had to generate a lot of data to populate a table and then generate data for a table that was joined to the first via a shared UUID key. So I wrote a closure that generated an array of UUIDs of a specified length and returned a generator function that would loop through the array and return to the 0th index after reaching the end. Each `.next()` call iterated and returned the next value ad infinitum. This allowed us to create as many tables with as many rows as we wanted where each row would be able to garauntee that it would have a valid foreign key for joining data.

Weirdly enough, this was also the first time in a long while where I used an IIFE. Since I didn't actually return the generator but the generators `.next` method so that the function could be used inline with others that were responsible for generating random fake data without any different syntax."
95,"I have a coworker who does most of their work in MATLAB but sticks with a C style way of writing things. It boggles my mind that some people refuse to learn easier/better ways to write code. A couple of annoyances I have seen include:           
- looping rather than using matrices.        
- storing a short list of related variables as var1, var2, var3, etc rather than putting them in an array.        
- putting strings in cell arrays rather than normal arrays.       
- using sprintf rather than strcat to append strings.            
- never using tables.       
- nesting cell arrays inside cell arrays.        
- reading files by repeatedly calling fgetl and parsing, rather than using readtable.       
- writing code as scripts and manually changing variables to fit inputs, rather than writing it as a function with those inputs.       
- not profiling their code to see why it takes forever"
96,"> The code runs faster

I dont use MATLAB much anymore, but once upon a time I had a code (that I didnt originally write) that took 20-30 minutes to run. When I finished vectorizing and optimizing everything it was running in around a second.

So yeah, it can make a HUGE difference.

Granted in this case the original code was so slow because it not only did the compute in a loop, but this loop involved continuously growing a non-preallocated sparse array on every iteration, which in turn required re-creating the sparse array and deep copying it to new memory addresses every iteration. But even after preallocating the sparse array adding vectorized compute still was a significant speedup."
97,"Interesting that you say it's easier to debug in vector format. I will actually often write code in loops with very specific variables called out and calling out the ith jth or kth of that variable. When things break I know where. Then I speed up that by vectorizing it, and go from running the variables over 1-10 and going 1-10000"
98,"That's a silly example for using matrices. Nobody would ever do it in the ""loopy way"" or the ""matrix way"". You'd simply type it into Matlab exactly as you wrote it: 

    sqft = [2104; 1416; 1534; 852];
    price = 0.25 * sqft - 40"
99,I started to work on my DFS optimizer. My first draft of the source code is now on [GitHub](https://github.com/nothans/dfs-optimizer) and I wrote a tutorial on my [blog](https://nothans.com/win-at-dfs-by-optimizing-your-fantasy-football-lineups). Let me know if you give it a try.
100,"What does the error code say? Matlab's error codes are generally quite descriptive and give you a good idea of what needs changing.

Without knowing that information, what version of matlab are you using? 

More recent version are a little less strict, but older versions are not a fan of unnecessary semicolons.

The semicolon after you end statement is superfluous. A semicolon at the end of a line suppresses console/command window output for the results of that line. An end statement does not have any results and, therefore, you cannot suppress any output from it.

Not sure if that'll fix your error though."
101,"In the assignment, I already wrote functioning code for calculating and plotting electric fields in free space, calculating 3D vector magnitude, and plotting 3D vectors using the Cartesian system. I cannot figure out why it is giving me such a hard time with this FOR loop, any guidance is much appreciated."
102,"Any optimizer. fminsearch, etc.

Fun fact - [mldivide](https://www.mathworks.com/help/matlab/ref/mldivide.html) (backslash operator) uses least squares for non-square problems. A great way to find ""another"" way to solve the same problem is read about how the original tool worked, and just duplicate that or change some part of it."
103,"You *could* be a smartass and do `x=mldivide(A,b)`. Technically `mldivide` is a more long winded way of calling the same routines as `\`, but also technically you're not using the operator.

As a serious answer, [Gauss-Seidel](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method) is usually the first method of solving linear systems that is taught in numerical methods classes. It's fairly easy to implement in just a fairly small handful of lines, and it actually works quite well."
104,"If you'll look in help of ""mldivide"", you'll find algorithm, how it works.

Or you can try this:  
`A = magic(3);`  
`B = [15; 15; 15];`  
`fun = @(x) sum((A * reshape(x, size(B,1), size(B,2)) - B).^2, 'all');`  
`x0 = reshape(B,[],1);`  
`x_ans = reshape(fminsearch(fun, x0),size(B,1),size(B,2))`"
105,In addition to what /u/TheSodesa said... You clearly have a working computer and clearly have an internet connection. Please make posts from your computer and just copy/paste your code instead of (presumably) using a mobile app and attaching multiple photos.
106,"Functions generally take an input and produce an output. The output of one function can be the input of another function. In pseudocode:
    
    outputA = function1(input);
    outputB = function2(outputA);

Functions can also call other functions from inside themselves. Or even call themselves, this is known as recursion. In pseudocode:
    
    output = function1(input);
    
    function function1(A):
       B = function2(A);
       output = some other operation on A or B or both"
107,"I don't know what you are trying to do, but generally speaking, yes, you can nest functions and there are way to share variables between functions. 

[https://www.mathworks.com/help/matlab/matlab\_prog/nested-functions.html](https://www.mathworks.com/help/matlab/matlab_prog/nested-functions.html)

[https://www.mathworks.com/help/matlab/matlab\_prog/share-data-between-workspaces.html](https://www.mathworks.com/help/matlab/matlab_prog/share-data-between-workspaces.html)

Good luck."
108,"I can't clearly tell what is and isn't commented, and the comments on your forceearth() and forcefeet() functions are identical/incorrect for one. If you are asking for help, it is a basic requirement to properly comment and format your code first. Asking someone else to decipher spaghetti comments because you don't want to fix the formatting is not going to get you help from most people, especially on reddit for basic questions. You could also benefit from looking at examples, as function syntax is a thoroughly explained topic on Youtube and mathworks.com. I'm gonna try to help anyways, but please take this advice seriously going forward. Professors will tell you to stop wasting their time if you ask for help and show them a codeblock like that, I promise you. Many of us have been there.

It looks like you're defining four functions to calculate
a) the force on a person's head at one earth radius
b) the force on a person's feet at one earth radius
c) the force on a person's head at one solar radius
d) the force on a person's feet at one solar radius
from a black hole of known mass.

Why? You only have one general equation, which calculates gravitational force between two bodies separated by distance r, applied four times using different values for M1, M2, and r. Just write a generalized function that calculates g (what you call y) based on constants G, M1, M2, and r. Then call that same function four times, passing in each of the four sets of parameters for each of the four situations a-d. Each function call should assign the generalized output to a descriptive variable to store the result. Then just pass those results into the subtraction function (which shouldn't be a function, I can't imagine your assignment asks you to write a function that literally just uses a basic math operator on the two parameters)"
109,"Great write up!

I often use cell arrays for compatibility reasons but the performance comparisons are convincing. Do you know why cell arrays take so much more space?

Just a nitpick for your example, you can indeed generate a cell array in a one liner, although it is a little less readable:

    fnames_cell = compose('yob%i.txt',years)"
110,"If you have an Excel file with a table like this - please note, I added multiplication operator \* i.e. 2\*A, not 2A

||A|
|:-|:-|
|1|2\\*A 2\\*B;C A\*B|

you cannot use `readmatrix`, because this is not numeric data. Instead, use `readtable`

    tbl = readtable('ex1.xlsx','ReadVariableNames',false,'TextType','string')
    tbl.Var1

Then you get a string value ""2\*A 2\*B;C A\*B"".

Then you can use `eval`.

    A=1; B=2; C=3;
    eval(""["" + tbl.Var1 + ""]"")

This will return \[2  4; 3  2\]

However, I don't recommend using `eval`."
111,">wondering if I could use something like fminsearch to automatically determine the optimal parameters for each subject

Yeah, in theory, why not?

>If I set [fminsearch] to something like 50 [iterations], is it feasible to get pretty good and consistent optimized parameters from that, if I have 4 parameters?

Maybe? It depends. Outside of convex problems using a very limited number of methods (maybe only one method, it's been a hot second since my last optimization lecture), there is no way to know for sure how many steps it will take you to to reach a minima in advance.

You can however set convergence/tolerance based stop criteria, in addition a limit to iterations. [Here is how you do that](https://www.mathworks.com/help/optim/ug/optim.problemdef.optimizationproblem.optimoptions.html)

>it is also unclear to me how fminsearch decides how to increment changes to the parameters

It does so using the [Nelder-Mead method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method).

>Can I make it change the parameters in larger increments to make up for the limited number of iterations? 

I've not used fminsearch enough to do that, but increasing step size in a blanket manner is generally not a great idea. There's a pretty non-negligible risk of never actually reaching a minima. Generally, most algorithms will include a criteria for determining an appropriate step size. Again, I haven't used fminsearch enough to know if its underlying algorithm does this.

> Also, some of the parameters will result in the function failing if they are too low or too high. If the function fails in one of my iterations, will that abort the optimization process?

In other words, your parameters need to be constrained. In this case you want to be using a constrained optimization function, such as [fmincon](https://www.mathworks.com/help/optim/ug/fmincon.html).

fmincon is a *gradient based* optimization function with a larger number of different underlying algorithms. It gives you the option to select the algorithm you want, or to let fmincon automatically select the algorithm it thinks is best. It helps to be able to provide the function with the gradient and Hessian of your function, but not necessary. If not provided, fmincon will compute these on its own using a finite difference. For just four parameters, this shouldn't take too, too long, depending on how your objective function is defined.

>Also, is there a way to easily make fminsearch output information on each iteration (inputs/outputs) as it runs? Is that what the PlotFcn option does?

I've never used PlotFcn, but it seems like that is what it's for. Personally, I've always used [this](https://www.mathworks.com/help/optim/ug/iterative-display.html) in my own work. Generally, I'd advise avoiding any sort of live plotting with matlab if computation time is a concern. While matlab has some pretty solid plotting options, it is *far* from computationally efficient."
112,"Nice! I'm an emacs user, and it is very rare that I open the main GUI window. Good luck developing this, and hope it works out! I love seeing more plugins to use MATLAB as a general purpose tool, and not a GUI-beast. I know there had been some changes on MATLAB's end regarding terminal access over the past few years, and I think its a flat no-go for windows users, but I'm not sure at all anymore."
113,"`pixelsTransparent= find(alpha==0);` gets you the pixels that are transparent. 

I'd just convert these pixel's values to 1 (or 255, not sure how matlab treats transparency values). Then I'd change the colors of these pixels to whatever color you're treating as neutral."
114,"You're running into the reason that you should be using functions instead of scripts for most of these things. Also, your scripts don't have to start with `clear all` meaning anything you calculate in one script would still be in your workspace when you called your second. 

But really, you should be using [functions](https://www.mathworks.com/help/matlab/ref/function.html) and handing in and out the variables you want to save."
115,"I only see one way that would be to compute simultaneously different step sizes in parallel. So this would not change the ode integration but only the step size adaptation. 
This would be only a minor improvement.

Mayber there would be better improvement possible with collocation scheme and BVP solvers where the IVP is computation distributed all over a time mesh. In that case the function has to be evaluated many times in parallel. Not that solving an initial value problem with a BVP solver requires in general an initial solution (that may be difficult to found sometimes) and this is not needed for explicit integration scheme."
116,"If you took the results of movmean(speed,5) from the 3rd element to the 3rd last, you should get the same results as your excel sheet, due to how movmean is centered.  Movmean(A,5) has a symmetrical sliding window of size 5, which means that values 1:5 will be centered on 3.  Your excel sheet has a non-symmetrical sliding window, with 0 before and 4 after. 

You can also use movmean(A,\[0,4\]) to emulate what your excel file does, but again you would need to treat the edge cases carefully."
117,"Most likely your amplitude is out of range for audiowrite.  
For single and double precision signals, audiowrite requires the signal to be normalized between -1 and +1.      
Check your signal's magnitude."
118,"Pretty much anything you can run on your desktop is available.

* MATLAB and Simulink Suite for Startups is the full desktop collection including all code gen.
* MATLAB Suite for Startups is all MATLAB-based toolboxes, deployment, and MATLAB Coder.

All server tools are sold separately on a la carte basis.

In addition, you also get

* Technical support
* Free online training
* co-marketing opportunities - videos, interviews, blog postings, and speaking roles at MathWorks events

Most startups in the program meet the following criteria:

* Founded within the last 5 years
* Fewer than 15 engineers
* Less than $1 million USD in annual revenue

(Angel/venture funding levels are not part of criteria for acceptance.) 

To get more details, apply for the program here

[https://www.mathworks.com/products/startups.html](https://www.mathworks.com/products/startups.html)

I hope this helps."
119,"> preferably without having to pay?

This is the ""easy"" part- if your school wants you to use MATLAB, it is almost guaranteed that they will have a MATLAB license for you to use. Most schools these days do. 

Secondly, if you want to learn how to use MATLAB, I have found there is no better way than just choosing a project you think sounds fun, but hard, and starting it. For example, since you're in Aerospace, maybe you would like writing an n-body orbit simulator. 

Make a function that you get to define the starting masses, positions and velocities of n-bodies (aka 2 to 10 or whatever), and then model how they would all orbit, and then decide to plot their orbits (or if you're feeling particularly motivated, animate their orbits). This will teach you how to store information, how to use MATLAB's ode solvers, how to plot, etc. 

I have always had better luck learning something by choosing a fun project then just going through a training."
120,"> Is there any way I can learn fast, preferably without having to pay?

The Mathworks website offers a Matlab Onramp and other courses you can take, if you have an academic Matlab license, offered to you by your school.

Other than that, there are no shortcuts. You need to code to learn to code. Just try to avoid writing scripts and start all of your programs with a main function:

    % In file +my_project/main.m

    function output = main(input)

    % my_project.main
    % 
    % The main function of this program.
    %
    % Inputs:
    %
    % - input
    %
    %   Description of input here.
    %
    % Outputs:
    %
    % - output
    %
    %   Description of output here.
    %

        % Argument validation block. List
        % arguments and their restrictions
        % here.

        arguments

            input

        end

        % Your program goes here. For now this
        % is just the identity function.

        output = input;

    end % function

Notice the symbol `+` in front of the mentioned project folder name. It makes it so your project becomes a *module* from Matlab's point of view. This means that your function has to be called with

    >> output = my_project.main(input);

on the Matlab command line. This prevents name clashes with other modules that also have a `main` function.

**Edit:** here is a link to documentation on functions: <https://se.mathworks.com/help/matlab/functions.html>.

**Edit 2:** another link on function input validation:  <https://se.mathworks.com/help/matlab/matlab_prog/function-argument-validation-1.html>."
121,"Compared to other languages, MATLAB is not too difficult to learn. Perhaps you can tell me where you are having problems?

MATLAB is based on linear algebra, so people who have coding experience in other language actually struggles because they try to do thing in scalar variables with loops rather than taking advantage of matrix computation. Since you don't have that issue, you are at an advantage."
122,"Code. Code. Code and code.

If price is a bit prohibitive or you have no access outside school, you can practice with Octave or Scilab. Both are _almost_ fully compatible with Matlab or at least code migration isn't impossible"
123,"First, do some of the MATLAB intro courses offered for free with student licenses. Your university almost certainly offers student licenses. Make use of it.

Second, use it for everything. I learned far more about the MATLAB language and scripting/coding using it for homework in grad school than I did in a dedicated MATLAB course during undergrad. I continue to learn so much about it using it in my day job."
124,"Hey there OP,

What I've found, is there's a lot of resources to 'learn matlab' but few that actually go into details for solving more advanced problems.

Firstly, if you need basics --> [https://www.youtube.com/watch?v=EtUCgn3T9eE&list=PLsLSMBRXdWJbh5x-f6sLvRTUlsrDTZnen&index=1&t=45s](https://www.youtube.com/watch?v=EtUCgn3T9eE&list=PLsLSMBRXdWJbh5x-f6sLvRTUlsrDTZnen&index=1&t=45s)

Secondly, challenge yourself to do these more complex algorithms (before watching how it's done) --> [https://www.youtube.com/playlist?list=PLsLSMBRXdWJabi2kPXvmx2mYjAxIxGPRM](https://www.youtube.com/playlist?list=PLsLSMBRXdWJabi2kPXvmx2mYjAxIxGPRM)

Practice makes improvement!"
125,"18 dimensional array interpolation?    For a dense space sampled as a 4x4x4x...x4 cube you will use more than 128gb of RAM

What are you actually trying to do, because it seems like this is just fundamentally modeling the problem incorrectly."
126,"I've never used pinv, but I actually ran into a similar problem recently. Coincidentally, I was also doing a kind of interpolation.

With that said, are you feeding a sparse matrix to pinv? If so, it's possible that the matlab implementation of pinv is not compatible with sparse matrices and is converting your sparse matrix to a full matrix. This was the problem that I was running into. 

However, is there a particular reason you are using pinv instead of mldivide (aka ""\"")? In my experience, mldivide is always faster than any other built-in solver and, contrary to what papers from younger researchers tend to claim, it will almost always use less memory. Of course, if you're after the unique properties of the Moore-Penrose pseudoinverse, that's another story. But short of using multigrid methods or compile mex functions that are optimized for a particular problem, it's very rare that mldivide can be outperformed by anything.

With all that also said vectorizing doesn't necessarily have an effect on memory usage, this is a common misconception. It mostly just helps with computation time. When you write loops, said loops have to be interpreted or compiled by matlab at runtime. Depending on what you're doing, these loops might have to be interpreted or compiled for every iteration. This is what causes them to be slow in matlab. Functions and or syntax that you use when vectorizing code will simply call already compiled C or Fortran based routines to carry out whatever operations you need. More often than not, the working parts of the source code to these routines will be more or less identical to the loops that you are replacing."
127,"I'm not familiar with append() and would just make a loop, however first I would look at the type of the origin column. I think that might be a categorical type and could be causing issues.

You can also look into the rowfunction() command"
128,"    N=1000; %10 samples per second for 100 seconds.
    NQ=60000; %60000 query points
    t=linspace(0,10.0,N)'        %set up the sample input column vector
    tQ=linspace(0,10.0,NQ)'      %set up the query input column vector
    y=randn(N,18)  %your data

    %this is what you should actually be doing.  You
    %are doing 18 1-d interpolations over time
    %interp1d can natively support this, it treats a matrix-input to Y as 
    %columns where each column is a 1-d var. Which is what you want.
    %the problem is therefore solved.   Don't use any of your code.  18-d RBF is the wrong solution.
    yQ_interp1=interp1(t,y,tQ,'makima');"
129,"Among other things, one reason why your code is failing is because inside RBF_K(xq,X) you are creating a 60000x1000 array using repmat.  The line XX1=repmat(XX,1,N2), XX is 60000x1, and N2 is 1000, so XX1 is 60000x1000.       

As was earlier said, you shouldn't use pinv to solve this, as pinv is going to fully compute a 1000x1000 SVD which will be extremely slow.  replace line 3 with `weights=G\y` which will still be the least squares solution, but it will be actually fast and actually numerically stable.

But really, as said before, you should actually throw out all of this code because RBF in 18-d is inherently the wrong way to solve 18 1-d interpolation problems."
130,"In addition to other things mentioned, I would also suggest to use ""DisplayName"" in the loop like this: (stealing the rest of the code from Creative_Sushi :-) )

    x = 1:10;
    y = [x.^2;2*x;pi*x];
    figure, hold on
    for ii = 1:size(y,1)
        plot(x,y(ii,:), ""DisplayName"", ""y"" + ii)
    end
    legend(""Location"", ""best"")

That way you don't have to worry about creating an array. You create the line with a name, and legend simply uses the name."
131,"If you want to handle strings, my recommendation is to use [string arrays](https://www.mathworks.com/help/matlab/ref/string.html), not cell arrays.

    str = strings(1,3);
    x = 1:10;
    y = [x.^2;2*x;pi*x];
    figure
    for ii = 1:size(y,1)
      plot(x,y(ii,:))
      hold on
      str(ii) = ""y"" + ii;
    end
    hold off
    legend(str,""Location"",""best"")

In this example, I started by creating an empty string array `str`, then filled each element as the plot is being built. Double quotation mark creates a string scalar string ""y"", and you can add a number to it using + operator and the number is automatically converted to string, like ""y1"".

Unlike Python, we avoid using append in MATLAB, and instead we declare a variable for memory pre-allocation.

If we don't know the size of the resulting array, we may use ""concatenation"" approach, but for a larger array this could be slower.

    str = [];
    x = 1:10;
    y = [x.^2;2*x;pi*x];
    figure
    for ii = 1:size(y,1)
      plot(x,y(ii,:))
      hold on
      str = [str, ""y"" + ii];
    end
    hold off
    legend(str,""Location"",""best"")

`str = [str, ""another string""];` concatenates a new string to the existing string array `str`. There is a command [append](https://www.mathworks.com/help/matlab/ref/append.html), but it doesn't behave like Python's.

I hope this is helpful."
132,"I’m late to the game here but I don’t think I saw this mentioned:

`num2str(s)` returns a **char** not a **string**, despite the misleading name. If you want a string, you need to explicitly make it one: `string(num2str(s))`.

Chars have single quotes, strings have double quotes, and that’s always how you’ll distinguish between them.

Strings behave more or less like how you’d expect them to in Python, as far as I can tell from my minor Python experience. Chars, on the other hand, are essentially…arrays of individual characters. That’s why you got ‘Function5913’ in your example. Imagine I define variables `a = [1, 2]` and `b = [3, 4]`. If I defined a new variable as `c = [a, b]`, then I would find that `c` is [1,2,3,4]. (In Python, the same arrangement would instead be [[1,2],[3,4]], so don’t get confused here.) Essentially your ‘Function5913’ under the hood looks like [‘F’,’u’,’n’,…’5’,’9’,’1’,’3’] for the same reason. Does that make sense?

`numel(‘myString’)` is equal to 8, the number of characters in ‘myString’.

`numel(“myString”)` is equal to 1, the number of strings. Had I instead done `numel([“str1”, “str2”, “str3”])`, it would be 3, since the array I fed `numel` has 3 strings.

I hope that clears a lot of this up. Shoot me a DM if you have any other questions. Been using Matlab a long time."
133,Are you referring to the Dynamic Load block? [https://www.mathworks.com/help/physmod/sps/powersys/ref/threephasedynamicload.html](https://www.mathworks.com/help/physmod/sps/powersys/ref/threephasedynamicload.html)
134,"I'd say this is a good basic history lesson on MATLAB as a whole, but I would add more content specifically about the language/IDE itself.  Maybe create a simple ""Hello World""-esque program to show the newbies."
135,"Generating random points inside of a rectangle is really just generating two columns of random numbers bounded between your x values and y values, and then putting those two columns together. 

Generating `n` random numbers between [a,b] is one of the first algorithms you learn in CS class

    rand_x = a + (b-a)*rand(n,1)

So, you could just do that twice, and then combine them. But, MATLAB works just fine doing it all at once:

    rand_pts = [x1,y1] + [x2-x1,y2-y1].*rand(n,2)

(where your `x`s and `y`s are the bounding x and y coordinates of your rectangle."
136,"If your region is a rectangle, then one possibilities is to map the unit square or disk to your bounded domain. Then generate a random number into the unit square or disk and use the mapping function to get something i side your domain. 

The most difficult point is to find the mapping... It is obviously trivial for domain such as boxes or disk or ellipsoid.. for more elaborated domain it may not be so simple."
137,"At the moment I don't have time to write a comprehensive answer, but feel free to ask if you'd like any elaboration.

Basically you're calling randi a bunch of times, and repeat this 1E7 times. Instead, you could create appropriately sized [arrays of random integers](https://www.mathworks.com/help/matlab/ref/randi.html#buf2dpy-3).

You call min 1E7 times as well. Instead, you could just use [min to find the min value along rows or columns](https://www.mathworks.com/help/matlab/ref/min.html#bupsfd_). Same goes for max.

This would get rid of your for loop.

I could give it some thought and suggest how to remove the while loop later, if you like."
138,"I was told that live scripts are slower than .m scripts as well. And I have also experienced trouble with them loading, where I have to restart matlab... I just use .m scripts now with %% to make different sections and it will be basically the same as a live script."
139,So you are making a 3d matrix from 8 2d cells after cell2mat? The for loop should help for extracting sequentially. Alternatively you can extract all variables and then cat the matrices and  use “reshape” to make it the size
140,"Here is a blog post about simulating Monty Hall problem in MATLAB, and the code example doesn't involve any loops, and gives you Bayesian explanation.

[https://blogs.mathworks.com/loren/2015/03/25/bayesian-brain-teaser/](https://blogs.mathworks.com/loren/2015/03/25/bayesian-brain-teaser/)

Good luck."
141,"Here's how I would approach it. The main change is to use numbers to represent which door is picked. So for instance, currently you'd represent the third door being picked like this: `R1 = [0, 0, 1]`. Instead if we say `R1 = 3;` to represent the third door being picked, we can now vectorize it more easily.

For instance with `n = 1E7` trials, you could represent all the player's guesses for the trials like `R1 = randi(3, [n, 1]);` Expanding this idea to the other rows too, you'd get something like this:

    n = 1e7;
    numDoors = 3;
    
    R1 = randi(numDoors, [n, 1]); % car location
    R2 = randi(numDoors, [n, 1]); % player pick
    R3 = randi(numDoors, [n, 1]); % revealed door
    
    % replace elements of R3 until they don't match R1 or R2
    toReplace = (R3 == R2) | (R3 == R1); % logical index of the ""reveals"" which need to be reselected
    while any(toReplace)
        k = nnz(toReplace); % number of reveals that still need to be replaced
        
        % The next two lines account for 70% of the computation time, so you
        % might investigate selecting the ""reveal"" in a smarter way to avoid
        % this whole loop
        R3(toReplace) = randi(numDoors, [k 1]);
        toReplace = (R3 == R2) | (R3 == R1);
    end
    
    doorIds = (1:numDoors);
    % min along 2nd dimension to find which door to switch to
    [~, R4] = min(R2 == doorIds | R3 == doorIds, [], 2);
    
    wins = nnz(R4 == R1);
    wins/n*100

This runs in 3 seconds or so, so not lightning fast, but hopefully gives you some ideas. Let me know if you want me to explain any part of it further"
142,"Let me do your HW for you. ;-) 

About 10x speedup and I didn't do much optimization or testing.

Things to learn:

.\* operator for doing thing where a condition is satisfied

OR can often be expressed as a +, AND uses .\*

Example:

N=1000000
  
a=rand(N,1);
  
b=rand(N,1);
  

  
% (a>.9) AND (b>.9)
  
(a>.9).\*(b>.9);
  
100\*sum((a>.9).\*(b>.9))/length(a)
  

  
% (a>.9) OR (b>.9)
  
(((a>.9)+(b>.9))>0);
  
100\*sum(((a>.9)+(b>.9))>0)/length(a)
  


&#x200B;

&#x200B;

&#x200B;

DD=\[\];
  
N=1E6
  
tic
  
DD=floor(rand(2,N)\*3)+1;
  
% row 1 car location, 1-3
  
% row 2 guess, 1-3
  
% row 3 revealed (not row 2 or row 1)
  
% two cases for row 3, either R1=R2 or R1\~=R2
  
% Case 1:
  
%DD(1,:)==DD(2,:);  % Are they equal?
  
%DD(1,:);  % values ?
  
%DD(1,:)+sign(rand(1,N)-.5); % randomly increment up down
  

  
% If R1\~=R2 reveal the other choice, if R1=R2 reveal random
  
DD(3,:)=(6-DD(1,:)-DD(2,:)).\*(DD(1,:)\~=DD(2,:));
  
DD(3,:)=DD(3,:)+DD(1,:).\*(DD(1,:)==DD(2,:))+sign(rand(1,N)-.5).\*(DD(1,:)==DD(2,:));
  
% If 0 or 4 is selected when R1=R2, correct it up
  
DD(3,:)=DD(3,:)+(DD(3,:)==0)\*3+(DD(3,:)==4)\*(-3);
  
% What should the person switch to?  R4 is new guess
  
DD(4,:)=(6-DD(2,:)-DD(3,:)) ;  % Switch to?
  
DD(5,:)=DD(1,:)==DD(4,:);   % winner? 1 for win.
  
100\*sum(DD(5,:))/length(DD(5,:))
  
toc"
143,"How did you obtain the velocity fields?

Are you calculating dynamic pressure? If so, you're calculating a scalar field per [the equation here](https://en.wikipedia.org/wiki/Dynamic_pressure), which stems from Bernoulli's equation... which itself stems from Euler if I'm not mistaken.

Your code would be along the lines of:

`P=0.5*rho*((sqrt((Vx.^2)+(Vy.^2))^2));`

The dynamic pressure gradient can then be computed using the built in [gradient function](https://www.mathworks.com/help/matlab/ref/gradient.html).

edit: forgot a parenthesis"
144,"I'm not familiar with queueing theory, but I took a quick look at the M/M/1 wiki page and your script. A few things that *maybe* are a problem:


* **exprnd**: In matlab's docs it says the first input to `exprnd` is the distribution mean `mu`. But on the wikipedia page they talk about a *rate* parameter (lambda) and **not** a mean parameter (mu). Is the `1.8` you're using with 
`exprnd` supposed to be the `rate`? If so, you should be doing `exprnd(1/1.8)` I think.

* **poissrnd**: So the input to this is indeed a rate, but I think the output is an integer representing how many customers arrived in a given time period. I think this is different than the output of `exprnd`, which represents service time, right? Again, I don't know anything about queueing theory, but if `A` and `poissrnd` indicate number of arrived customers, it looks weird to me that the script seems to treat `A` like a time.

So maybe something like this would work?


    poissrate = 1.2;
    ...
    while completed < stopcriteria
        clock = clock + 1;
        newarrivals = poissrnd(poissrate);
        queuelen = queuelen + newarrivals;
        ... etc
    end"
145,"`n` is a 1x80 vector from 1 to 80.

what are you trying to do with `j`?   The syntax `(1:n)` only makes sense if `n` is a scalar, but it is a vector. 

You also have a for loop that iterates from 1 to 1000 using `M`, but `M` is never used inside the loop. 

Since `N` is a scalar and `n` is a 1x80 vector, `pn` is also a 1x80 vector, but the value doesn't change in 1000 iterations. 

When I do `pn(22)`, I get 0.0575. I think you are missing something in your code that supposed to change through iterations. 

Good luck."
146,"It looks like you're trying to run the ""birthday experiment"" and estimate the probability that any two people share a birthday given a certain number of people. Here's an outline (with some parts left unfinished) that might get you started:

    numDays = 365; % days in a year
    numTrials = 1000; % number of times to run experiment for each # of people
    maxPeople = 80; % max number of people to test
    
    pn = zeros(maxPeople, 1); % initialize pn
    
    % iterate over different number of people
    for numPeople = 1:maxPeople
    
        % matchCount keeps track of how many of our trials have people with the
        % same birthday
        matchCount = 0;
    
        % run the experiment 1000 times
        for i = 1:numTrials
            % ...
            % your code here
            % ...
            matchFound = ????; % fill this in
            if matchFound
                matchCount = matchCount + 1;
            end
        end
        probability = matchCount/numTrials;
        pn(numPeople) = probability;
    end

I'll leave it to you to fill in the `your code here` and the `????` sections. As a hint, you might want to look up the `randi` function. For instance, you could generate a random integer from 1 to 365 for each person to represent their birthday. 

And then once you've generated a random birthday for each person, you'll need to check if there are any two that match. I'd recommend the `unique` function for that."
147,"Hello, [format longE, longG, etc.](https://www.mathworks.com/help/matlab/matlab_prog/display-format-for-numeric-values.html) changes the display format of numeric values for the current MATLAB session, not just for one table or another. Therefore, it is not possible to do so for a single column.

By the way, `fprintf` is part of a very old fashioned way to do things and I haven't use that command over 5 years. Unless you have specific reason to use it, I would stay away from it.

Usually, when we say plot, we are talking about plot command, which displays a graph in x-y coordinates. If I understand correctly, if are just printing values of a table.

If you want to print the content of the single column `myCol` of a table `tbl`, all you need to do is `tbl.myCol` and you see all the data in that column returned.

If you want it to be in a specific format, you can use [compose](https://www.mathworks.com/help/matlab/ref/compose.html), and this will give you all the numbers in 2 decimal points.

    compose(""%.2f"",tbl.myCol)

Good luck."
148,"For sake of simplicity let us assume that the you have amplitude versus time for your signals.  
Wouldn't you want the two signals values at the same time to add?  
One signal is defined from 0 to 9 and the other -2 to 5.  
I think it would make your life easier if you took those signals and defined them on a common axis...that is -2 to 9 or changed your functions so you could define the output axis."
149,"I think you could use `interp1` for this. It takes the original x, y data as well as a new x-vector and spits out a corresponding y-vector. `yi = interp1(x, y, xi);`

You'll need to specify the interpolation method (nearest) and the value it uses for extrapolation which you said you want to be `0`. So it would be `yi = interp1(x, y, xi, 'nearest', 0);`

    function [y,n] = sigadd(x1,m1,x2,m2)
        n = min(m1,m2):max(m1,m2);
        x1i = interp1(m1,x1,n,'nearest',0);
        x2i = interp1(m2,x2,n,'nearest',0);
        y = x1i + x2i;
    end"
150,Do you want each matrice to be a separate variable? You can use the eval function to dynamically create variables but this is a bad practice. Can you not just use the matrices by indexing the cell array?
151," >I think the answer should be 1/2. Why is matlab ouputing this?

Because you told it that you didn't want a numerical answer. You used symbolic variables, so your answer will be symbolic. If you're trying to get a number, *use numbers*."
152,"Read the *Basic answers* -page on the site <https://floating-point-gui.de/> and your question will be answered. Write

    format long;

before the lines in the code you showed to get a better idea of what is happening."
153,"The <= are underlined. Move your mouse and see what the problem is, but my guess is that is not valid syntax for piecewise.

Perhaps also the first function argument must be a one-sided less than? But you can hover over it and see what it says the problem is."
154,"I'm assuming daughters boyfriend is going to be using this for school? What is his major? Undergrad or graduate level? I think the Microsoft laptop would be good enough for a bachelor level curriculum with matlab. It wasnt until my graduate level classes I needed 32-64 gb of ram. My experience was for electrical engineering though. I've done plenty in matlab on much worse laptops than either of those two, ymmv."
155,"I've heard that particular processor is really slow, like one of the worst. Probably fine for basic algos on matlab, but I usually export intense stuff to a powerful desktop anyways. Mac has always been in a weird spot. They tend to be more reliable, but users in school and engineering always run into compatibility problems because there's a lot of engineering softwares that require windows.

No matter which way you look, it's like every brand has some major pros and cons. I'd say get something cheap with a really good processor like 1270/1280p, like something from MSI or ASUS. Their main board configurations are usually much higher quality, can withstand much higher temps, run more efficiently, don't have bullshit like requiring a special charger with a special registered chip, and have all the necessary ports and features you'd typically need. 

IIRC the Microsoft Surface doesn't even have USB A ports or an HDMI out. Just 2 C ports and micro SD, that's it."
156,"Both are pretty much the same performance, probably i7-11th a little better since I have compared the MacBook to my i7-10th with similar results.

If he has an iPhone I would recommend the mac, for productivity.

I prefer to have windows laptop because I have a windows desktop PC. But I do have an iPhone and miss some of the nice functionality in the mac ecosystem."
157,"I would definitely go with the 14” MBP since they were on sale recently for $1600.

I used Matlab for my nuclear undergrad in 2013/14 on a 2010 13” MBP and it had plenty of power then. I would take all the benefits of a Mac (screen, trackpad, build quality, battery life, weight) over potentially saving 10 seconds on a calculation once a month. And once Matlab release an M1 native app then it will probably smoke the competition.

If he wants a gaming laptop that’s a different story"
158,"Have you tried solving the problem (or parts of it) by hand first? That can often help you determine the mathematical expressions you need, then the problem just becomes translating those equations into MATLAB"
159,"I think you’re getting the right answer. Look at the scale of your plot. The Y-axis spans only four units while the X-axis spans 30 or so. After the plotting, include the command “axis equal” You’ll see the angle you’re looking for is way bigger than 90 degrees. Here’s something else: you only have to use “hold on” once. It sticks until you use “hold off”"
160,"The time age is not loading to this is my best interpretation of that you are attempting.

When a function is defined at the top of a script, the whole script (.m file) is used for that function. So you can name the file the function name and use that function in other scripts or in the command window.

When functions are defined at the end of the script they are local to that script so that function can be used thought the file that it is defined in. These functions cannot be used in the command window, they only exist locally in the file."
161,"So, yes I can in fact just use the cells, no idea why I was trying to convert them... But my next problem (yes I'll also be making another post) is how to plot all of the data within the cell array onto a plot3. Again I'm wanting this to work no matter the number of cells there are in the array. Any advice?"
162,"I cannot explain why local functions must be declared at the bottom of the script, but I can suggest a work around. 

save your function in a separate function file, say 'mymean.m' and to show its content in a script, you can use type command. 

    type mymean.m

This way you can include the content of your function anywhere in the script. 

Good luck!"
163,"If you are building your own deep learning network, check out Deep network desinger app. [https://www.mathworks.com/help/deeplearning/deep-network-designer-app.html](https://www.mathworks.com/help/deeplearning/ug/define-custom-deep-learning-layers.html)

I also recommend Deep Learning Onramp to get comfortable using MATLAB for deep learning. https://www.mathworks.com/learn/tutorials/deep-learning-onramp.html

Good luck!"
164,"[https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.fullyconnectedlayer.html#:\~:text=Fully%20Connected%20Layer-,A%20fully%20connected%20layer%20multiplies%20the%20input%20by%20a%20weight,neurons%20in%20the%20previous%20layer](https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.fullyconnectedlayer.html#:~:text=Fully%20Connected%20Layer-,A%20fully%20connected%20layer%20multiplies%20the%20input%20by%20a%20weight,neurons%20in%20the%20previous%20layer). It is named fullyConnectedLayer in MATLAB."
165,"If `h` is an array of handles, then `get(h, 'parent')` returns a cell array the same size as `h` containing the parent of each of the handles.

Maybe the new axis you added got appended to `h` so now it's a 1-by-2 array of `axes` handles?

Easiest solution might be something like this:

    setpos(h(1))
    setpos(h(2))
    etc...

Or store the handle for your new axes in a separate variable"
166,"The easiest way to debug is to write a 'try-catch' around the piece of code that is throwing the error, then place a breakpoint after 'catch', so it only triggers when an error is thrown. You can then verify the class (type) and contents of your variables to understand what the problem is."
167,"Are you sure the sign is right on the subtraction block between `Mm` and `Ml - Mm`? I would've expected it to be `Mm - Ml` ...but I don't know a lot about motors.

Just surprising to see in your plots that `Vs` is positive while `Im` is negative"
168,"Hi u/emixato

Can you provide the transfer function of the mechatronic system? We can mathematically analyze if the PI controller is designed appropriately. After that, we can compare if the Simulink model is constructed properly."
169,"**UPDATE:**

The angular velocity now tends to the reference value; the problem was one of units.  
The error also now tends to approximately zero.  
The only concern I have is whether the values of the controller parameters P=100 and control\_I 0 0.001 are valid.  
The professor did not give us a method to calculate them so by trying many different combinations of values I arrived at this pair of values.  
Can you suggest how to improve the values of P and control\_I?  
Or are the set values (P=100 and control\_I = 0.001 ) correct?  
In any case, you can find the new results [here](https://imgur.com/a/KDZzq0o).  
While [here](https://imgur.com/a/AHRR1Oj) the full script."
170,"[Neural Network Fitting App](https://www.mathworks.com/help/deeplearning/ref/neuralnetfitting-app.html) is for regression problems that have no time component. So that would be any problem where you have some predictors, and you want to predict one (or several) continuous variables.

[Neural Network Time Series App](https://www.mathworks.com/help/deeplearning/ref/neuralnettimeseries-app.html) is for time series regression problems, which come in 3 formats.

* Predict series y(t) from past values of y(t).
* Predict series y(t) from past values of another series x(t).
* Predict series y(t) from past values of y(t) and values of another time series x(t)

If you type in ""help nndatasets"" in MATLAB, you will see a list of datasets grouped by category. Datasets in the category ""Function Fitting, Function approximation and Curve fitting"" will work with Neural Network Fitting App, and datasets in the categories ""Input-Output Time-Series Prediction, Forecasting, Dynamic modeling Nonlinear autorgression, System identification and Filtering"" and ""Single Time-Series Prediction, Forecasting, Dynamic modeling, Nonlinear autoregression, System identification, and Filtering"" will work with ""Neural Network Time Series App"".

Hope this helps."
171,"I don't know how to fix this but Matlab central might be a good place to check as well.

If you have a series of commands that works to detect the GPU you could add them to a startup script. This could work as an intermediate fit."
172,"Maybe this is what you are looking for?

    f = @(x,y) x + Y^2;
    x = 2;
    y = 1:100;
    z = f(x,y);
    diff_y = diff(y)./y(1:end-1);
    diff_z = diff(z)./z(1:end-1);
    figure
    plot(diff_y, diff_z)"
173,I was never able to get the GPUs to work properly for neural networks. The only suggestion I have is use python. That’s probably not much help but over the last ten years of using matlab I am 110% positive that if I had to start over again I would use python. The licensing fees are a stand-alone argument but matlab is also a piece of junk. Just my opinion.
174,You must first trim your system at an operating point and then linearize your model therefore obtaining a state space model with your A and B matrices. These can be used to design your LQR controller. Hope this helps.
175,"t = temp\_min:2:temp\_max;  
Then use the equation to evaluate Cp for all values of t.  
(Note: ensure to change '\^' to element-by-element exponentiation '.\^')  
You could also use a for loop and calculate it one-by-one"
176,"Hello,

The point of MATLAB is to use linear algebra.

Let's say there is an equation `y = a*x + b`, and you have bunch of constants for `a` and `b`, like

    x = [1;2;3];
    a = [-0.25 -0.20 -0.15;
    b = [1 1 1];

Then you can do this in linear algebra.

    y = x*[a; b];

Another thing to note is that you can generate a vector like this.

    temp = min_temp:increment:max_temp

I hope this helps."
177,"You can do bitshift, bitand, bitor, bitxor, etc. in Matlab. For multi-byte numeric types you can use swapbytes to change endianness. Alsp if the data is evenly broken up into bytes, you can just use typecast."
178,"You could try writing your own function `to_double` that performs the conversion and see whether it is faster. The asymptotic complexity of such a conversion should be linear in the size of the byte array at worst:

1. go over sequences of bytes 8 at a time,

2. cast each byte in an octet to a `double` (a 64-bit floating point number) so they can be shifted by 56 bits at worst,

3. shift each byte in the octet by (0, 8, ..., 56) bits or (0, 1, ..., 7) bytes based on their significance in the given endianness [×] and

4. mask the shifted bytes into a single double with a logical `or`/`xor`.

The shifting and masking can be done in a single `for`-loop.

[×] An endianness signifies whether the least significant byte is the first or last element of a byte sequence. A big-endian sequence has the most significant byte of the right, whereas with little-endian sequences the opposite is true. This affects the direction the bit/byte shifting should be performed in."
179,"It depends on your background and how quickly you learn new stuff, but with proper preparation and taking advantage of all the functionality available, you should be up and running fairly quickly. 

You probably want to start with free tutorial that doesn't assume any prior experience. [https://www.mathworks.com/learn/tutorials/matlab-onramp.html](https://www.mathworks.com/learn/tutorials/matlab-onramp.html)

Good luck!"
180,"I would imagine you’re probably in your late 20s? I did the same thing, going back after working for a while. It’s a tough slog, but it’s so worth it.

Matlab can be scary. Take it one step at a time and reach out to people and ask a lot of questions.

Have fun with the new journey!"
181,"Matlab has great documentation. You can google virtually every problem you might encounter. The sheer amount of time you might spend googling things might be discouraging, but it is very normal to do so."
182,MATLAB is such a great language for science/engineering because it lets you focus on the math and not on the programming. You can pretty much just spend a few days learning the basics then just dive right in to your problem and learn the rest as you go.
183,"If you’re not worried about the absolute values of the curves, and you only want to compare the relative change, you could normalize them to some arbitrary reference value. 

A typical choice of normalization value is the initial value (ie, the y-value for the first x-value of your dataset). So if you have two curves, y1(x) and y2(x), that begin at x=0, you could plot y1(x)/y1(0) and y2(x)/y2(0). Both of these will be 1 initially, and the curves represent the relative change. You can always quote the values of y1(0) and y2(0) so that the audience is aware of the scale difference."
184,"Provide some example inputs and outputs. It's unclear what shapes `x` and `xk` have.

`np.newaxis` is used to add dimensionality to a given array, while `np.swapaxes()` does exactly what its name suggests--it swaps two axes."
185,"You can use [norm](https://www.mathworks.com/help/matlab/ref/norm.html) to get Euclidean Distance.

    x = rand(10,1);
    xk = rand(10,1);
    distance = norm(x-xk);

Of course you may be doing this as a practice problem. If so, please disregard."
186,"Eww, don't do generative design. Do [topology optimization](https://www.topopt.mek.dtu.dk/apps-and-software) instead.

With that said and speaking from experience, it'll take you about 2 years to go from undergrad/typical masters knowledge to the point where you're meaningfully optimizing CAD geometry with any degree of efficiency with self-written code."
187,"Without seeing your data, it's hard to know if there's a problem or not. Depending on values

    (concentrationB - concentrationC) ./ (concentrationB + concentrationC)

can be positive or negative, and doesn't necessarily grow with X and Y"
188,"I'm still confused on the ""tables vs structs"" debate, since things I use tables for I would never consider using a struct for, and vice-versa, but I'll accept that I am the outlier. 

However, I didn't know about ""group summary"" an that's pretty sweet. I frequently have a lot of calls to `unique` on columns of table data, followed by counting by each unique element. It's pretty nice there's a single call that will do it."
189,"View in your timezone:  
[9/1 11 am EDT][0]  

[0]: https://timee.io/20220901T1500?tl=YouTube%20livestream%209%2F1%20(11%20am%20EDT)%3A%20Develop%20Autonomous%20Algorithms%20using%20ROS%20with%20MATLAB"
190,"Although M1 is great in terms of CPU usage (incredibly fast and efficient), unfortunately pytorch GPU usage is out of the question until developers release a pytorch with M1 compatibility. According to their github page, they are currently working on providing GPU support but it'll be most likely months until they release a beta version - which will likely contain many problems. I think it will take at least a year or two to have a fine working, error-free GPU support on M1 for pytorch. I got an M1 machine without doing much research into this, and now I'm stuck with whatever GPU google colab provides, which isn't much. Thus, cannot complete my research or have any good progress even. And I don't want to switch to tensorflow either.

So, if you must use GPU on your laptop when coding in pytorch and you cannot wait for a year or so for the GPU support, you should get a laptop with compatible NVIDIA card."
191,"1. As others have said with the GPU shortage you should check and make sure the workstation cards (e.g. A4000) aren't a better deal
2. If not then at the price of a 3080Ti I really would consider whether you can squeeze a 3090. VRAM is a hard limit to overcome, the 24GB 3090 will probably have much better long-term usability prospects than the 12GB 3080Ti and usually there isn't much price difference between them.
3. You can't use CUDA 9 with a 30 series card, has to be CUDA 11. You'll either need to build PyTorch 1.1 from source for CUDA 9 (if you can, I'm not sure) or upgrade your PyTorch version to something more modern.
4. Similarly, I'd strongly recommend against installing Ubuntu 18.04 at this stage. It's like 4 months from EOL. If you want LTS use 20.04 which is good for another couple years"
192,"I would recommend checking out the Pytorch forum for notes about eGPU compatibility. But the eGPU is the part here that I would be most concerned about being problematic.

On the GPU side, I would also take a look at the Nvidia RTX A4000 cards. You can find them for only about $1200 USD, they work great, and they have 16GB of VRAM. 

I'm assuming that your software versions you've listed is what you've been using? Although more than of those items are outdated at this point... Ubuntu, Pytorch, CUDA, etc. Any reason you can't update?"
193,"This is a Fakespot Reviews Analysis bot. Fakespot detects fake reviews, fake products and unreliable sellers using AI.

Here is the analysis for the Amazon product reviews:

>**Name**: MSI Gaming GeForce RTX 3080 Ti 12GB GDRR6X 320-Bit HDMI/DP Nvlink Torx Fan 3 Ampere Architecture OC Graphics Card (RTX 3080 Ti Gaming X Trio 12G) 

>**Company**: Visit the MSI Store

>**Amazon Product Rating**: 4.2 

>**Fakespot Reviews Grade**: A

>**Adjusted Fakespot Rating**: 4.2

>**Analysis Performed at**: 12-13-2021 

[Link to Fakespot Analysis](https://fakespot.com/product/msi-gaming-geforce-rtx-3080-ti-12gb-gdrr6x-320-bit-hdmi-dp-nvlink-torx-fan-3-ampere-architecture-oc-graphics-card-rtx-3080-ti-gaming-x-trio-12g) | [Check out the Fakespot Chrome Extension!](https://chrome.google.com/webstore/detail/fakespot-analyze-fake-ama/nakplnnackehceedgkgkokbgbmfghain)

*Fakespot analyzes the reviews authenticity and not the product quality using AI. We look for real reviews that mention product issues such as counterfeits, defects, and bad return policies that fake reviews try to hide from consumers.*

*We give an A-F letter for trustworthiness of reviews. A = very trustworthy reviews, F = highly untrustworthy reviews. We also provide seller ratings to warn you if the seller can be trusted or not.*"
194,"Either one folder is having more images or some images are having more than one channel

To isolate the problem loop over the items in the dataloader with batch size 1 without shuffle and print the shape of the array you got. Then investigate the ones with different sizes"
195,"Running out of memory is normal in any model you train. Your model is going to use up a lot of memory as you are working with large channel sizes. So this is a standard error we all run into and we reduce batch sizes accordingly for the model to fit into video (or CPU/RAM) memory. This is unrelated to your real problem. Your code simply isn't reaching the real error because it cant fit all that data into memory. I would suggest keeping an eye on your video memory/vram when you hit play on your training to get an idea of whats going on memory wise.

On to your real issue.

Your dataloader stacks images/scans on top of each other. With batch size of 1, none of your MRIs have to train in a stack. It doesn't matter if every piece of data has a different number of channels to the dataloader if theyre being loaded one at a time. If you want a batch size greater than 1, then the dataloader picks out (often randomly) x amount of folders (where x is the batch size) and stacks the data on top of each other. If one of the x images has a different number of channels, things dont line up in the stack and it breaks. So it seems like all your data is not all 160 channels. Maybe try to print the folder name in your dataloader code so that when it crashes you can identify what folder has the extra channels/images. Or you could do a check in your dataloader code to pause or print something if the channel size is incorrect. Another idea is to simply run through the entire dataset with a batch size of 1 and print out the channel size for each scan. You will either find that the scans are not all the same size channel as you first thought, or they are all the correct size and the problem isn't in your dataloader. If they are not the same size then either your dataloader is doing something weird (maybe appending channels from one folder into another folder because of a mistake but I don't know how this would be possible) or your data isn't as you thought it should be. 

One thing that still doesn't make sense though is that even if you use a batch size of 1, and you skip the dataloader problem, your model still expects an input channel size of 160. It doesn't make sense then for your model to run (for more than a complete epoch) without crashing once it reaches the scan with 163 channels. Does your model actually run for multiple epochs with a batch size of 1? If it does then perhaps your dataloader has a mistake in it and is mixing channels/images between folders. Again I'm not sure how this would be possible with conventional dataloading methods.

Lastly:

- Are you using a debugger in your coding environment? 

So you should be able to at least print out the channel sizes when running for batch_size=1 and go from there. Just comment out your model."
196,You dont have to fit all data into memory. You can subclass the Dataset class. Implement `__len__` and `__getitem__` and dynamically load your samples from disk (or from whereever) in `__getitem__`. You can pass that Dataset to DataLoader which will prefetch the data in time so that your process isnt waiting for the data (dont forget to set an appropriate `num_workers` in DataLoader)
197,"> AMD w 12GB RAM which has 0 pytorch support

Not true, depending on the card you may be able to use the ROCm build of pytorch.

But that's actually unrelated to your problem. You should be subclassing dataset and/or defining transformations, not loading your entire dataset into memory and pre-engineering all features. Subclassing dataset should let you load data just in time, and then if you don't have space to precompute all your features and cache them to disk you can just define transformations to apply on the fly."
198,"A couple questions:

What is your GPU usage looking like from nvidia-smi?  You may have some kind of issue with the dataloader constructing tensors, or too small a batch size to fully use the GPU.  You should be seeing like 99% GPU utilization with this I think.

Why did you pick a learning rate of 0.01?  That feels high to me, like it may overshoot and flap a bit.  Did you do some learning rate discovery, or just pick it at random?

Why a batch size of 64?  For such a straightforward model, I would expect to be able to feed more data at a time, which would make convergence happen faster.

I think there may be an issue with the way you are adding up the running loss, like it may be leaking gradients.  I think you may want to detach() the loss before adding it to the running loss, but I'm not sure.

Lastly, and this is probably not an issue since you seem to be hardcoding the number of epochs at 2, but are you shuffling the data in your dataloader?  If you have sorted data it may make convergence difficult.

In general though, I feel like you should expect sklearn to be faster on this as it has very specific solvers that are optimized for this problem.  I don’t think the time difference should be as extreme as you are seeing, but I think I would be surprised to see this specific training loop actually outperform sklearn even with GPU acceleration.  An optimized implementation of SAGA or SAG in PyTorch however, I could see working out better."
199,"I know it's irrelevant to what you're trying to solve, but could you also post the sklearn variant? I'm running something similar and could benefit from seeing your implementation.

As for pytorch problem, from my limited experience - bert/transfer learning, couple categories - that sounds like a pretty rough runtime. I'm sure others on here will give you the insight you need."
200,"Super explicitly you can do:

doc_product = (Y.unsqueeze(1) * X).sum(dim=-1)

There's probably a more efficient way but this definitely works. What it's doing is expanding Y so that it has shape (200, 1, 300), and then everything else works cleanly."
201,"maybe this? [https://pypi.org/project/sclblpy/](https://pypi.org/project/sclblpy/)

parent article: [https://towardsdatascience.com/why-would-you-use-webassembly-to-put-scikit-learn-in-the-browser-77671e8718d6](https://towardsdatascience.com/why-would-you-use-webassembly-to-put-scikit-learn-in-the-browser-77671e8718d6)"
202,"For people arriving here via Google, note that the successor to ONNX.js is ""ONNX Runtime Web"": [https://github.com/microsoft/onnxruntime/tree/master/js/web](https://github.com/microsoft/onnxruntime/tree/master/js/web)

This doesn't directly answer OP's question, but thought I'd mention it RE the ""Onnx.js is way way behind"" comment. The wasm backend seems to be fairly robust - has good op coverage and works for a decent portion of ONNX models ""out of the box""."
203,"You might wanna try to use static word embeddings like Word2Vec or GloVe. This article explains how to implement it with SpaCy: [link](https://towardsdatascience.com/how-to-build-a-fast-most-similar-words-method-in-spacy-32ed104fe498)

The python package whatlies might also be interesting to you as it has builtin support for retrieving most similar words"
204,"Ydata synthetic is a library that includes a TimeGan to generate time series data.
https://github.com/ydataai/ydata-synthetic

You can also try SigCWGans:
https://github.com/SigCGANs/Conditional-Sig-Wasserstein-GANs"
205,"You need the data going into resnet18 to be formatted like this:

Grayscale = [B x 1 x H x W]

Colour = [B x 3 x H x W]

You have a different scheme than this so you will have to hack around to change it. In my mind you will probably want to take your 3D DICOM channel of size 160 and place that in the colour/channel position.

You will then have [2 x 160 x 256 x 256]. This is actually what your dataloader is already outputting so thats good. 

Next you need to **replace the first layer of resnet18 to have 160 channels** (of your MRI stuff) instead of the rgb space. You'll probably find some examples of this on the pytorch forum. You basically just replace the conv operation which is the first in the list of the preloaded model with your own 2d conv of the correct size.

So to recap, you don't need to change the shape of your dataloader output, only the number of input channels resnet expects. [Heres a good example](https://discuss.pytorch.org/t/how-to-change-parameters-of-first-layer-in-pretrained-network-by-copying-its-own-weight-iteratively/65888)

    your_model.features[0] = nn.Conv2d(160, 64, 7, 7, 7)

Something like this should work. i.e. 160 input channels, 64 output channels (as resnet 18 expects for the subsequent layers). I presume theres a wide stride of 7 for the first layer and then 7x7 kernel size for the first layer. Think of it as modifying the network to accept 160 grayscale channels. 

Get it running like that first but next you might want to change up the network more because youre starting off with 160 channels and immediately squishing it to 64 channels to fit into resnet18. Maybe a larger version of resnet would be better or modify the following layers to also have more channels."
206,">The shape of the images in my dataloader is \[2,160,256,256\] where 2 is the batch\_size, 160 is the number of dicom images for each patient and 256x256 is the dimension of the images

Batch \* Channels \* Width \* Height is a 2D input not a 3D input. You should have Batch \* Channels \* Something \* Width \* Height where ""Something"" is the third channel you want to slide the filters over, typically something like depth or time but I don't know what's common in medical imagery.

From the weight shape (64, 3, 7, 7, 7) it appears that it expects 3 channels on the input, so probably RGB. So your shape should be Batch \* RGB \* Something \* Height \* Width"
207,Pytorch lightning has build in pruning and quantization callbacks https://pytorch-lightning.readthedocs.io/en/stable//advanced/pruning_quantization.html you could test different approaches with config files https://pytorch-lightning.readthedocs.io/en/stable//common/lightning_cli.html and create wandb sweeps https://docs.wandb.ai/guides/sweeps or use hydra https://hydra.cc/docs/plugins/optuna_sweeper/
208,"In the SSD torchvision code there is an if function that it returns the loss in training and the detections im eval mode. You could wrap the model call in torch.no_grad() and keep it in training mode so that you get the loss, but you would not be able to call the backward function than."
209,"So MaxPool2d(1,1) will use max pooling with kernel size of 1 and stride of 1. At each kernel placement the input is one vector and it's supposed to output one vector. The input tensor and output tensor will be the same as the result of max pooling in this case.

In contrast, MaxPool2d(2,2), which is using kernel size of 2 and stride of 2, will do more of what we might typically expect and decrease the width and height dimension by a factor of 2. At each kernel placement the input will be 4 vectors and output will be one vector."
210,"Hi,
I think you will have to use onnx to do it. You can transform your tensorflow model to onnx with tensorflow-onnx, then load it to Pytorch (with onnx2pytorch). But know than Pytorch is not good at loading onnx models."
211,"Have you been able to easily calculate test loss for the model? I have been struggling to do this on a test dataset. 

https://www.reddit.com/r/pytorch/comments/xgwi9c/calculating_loss_for_on_a_test_dataset_with_ssd/"
212,"You need to use this if you dont have 10GB vram

[https://github.com/basujindal/stable-diffusion](https://github.com/basujindal/stable-diffusion)

Also disable the safety checker model to save about 1.3GB(just comment out the model initiation and call) and change var name"
213,"If you don’t care about inference speed, then libtorch is the easiest way. If inference latency is the highest priority then do pytorch -> onnx -> tensorrt. Or using onnx runtime might also be a good middle ground."
214,"I am doing my PhD on timeseries forecasting and use Pytorch (recently started using darts too).

I am using hydra and grid searching for HP tuning. Timeseries models tend to be much smaller and have fewer HP than NLP/CV SOTA so fancier methods aren't always required.  

I have TBs of data but find that quality is always better than quantity."
215,"I would do the following:

1. Re-build one of the Keras or TF projects using PyTorch

1.a. Consider restricting reference material to only the PyTorch docs 

2. Compare the performance between frameworks on the same task

3. Write about what you liked and disliked about 1. Write about your experiment results in 2."
216,"It's hilariously easy to learn. The documentation is up to date and good. Follow the tutorials link, walk through one to understand the basic structure (which is very simple) and then re-implement your TF/Keras project."
217,"You can use log_softmax just before max function in the test loop since you use crossentropyloss.

You should use learning rate scheduler to decrease your learning rate properly.

Also training loss doesn't mean anything. Accuracy can be low when training loss goes to 0 (0 is not logical but I think you get the point). I suggest you to use validation for each epoch so you can avoid underfitting or overfitting."
218,"Ok I'll take a stab at this even though I don't really have much experience with VAEs.

A couple unrelated comments first:

1. No need to use a hyperparameter for each term. Just 1 is enough, which controls the *relative* weights of the terms. E.g., `total_loss = recon_loss + beta*kl_loss`, since scaling the total_loss is meaningless, and less knobs to turn the better.

2. It looks like you pasted code for the reparameterization trick inside a VAE. Essentially, to perform a reconstruction, a VAE would do `x -> encoder -> (mu, log_var) -> reparam -> z -> decoder -> x_recon`. To generate new samples though, you don't need most of that. Just sample some z's from the standard normal and feed it through the decoder: `return decoder(torch.randn(batch_size, latent_size))`. Your question is very reasonable though, so maybe you already understand this and pasted the wrong code or something, I dunno.

Ok so your question. I believe that technically a ""true"" VAE does not have those hyperparameters. Then, your loss term is the (negative) ELBO, which provides a lower bound to the marginal data distribution, and all is well. Introduction of the hyperparameter totally changes this (although it still may work from an optimization/regularization perspective), so I don't think we can have any guarantees about the distributions we'll be sampling. You're sort of gradually turning the VAE into a vanilla AE, but then asking how to sample from it, when a huge part of VAEs over AEs are allowing you to sample from them.

That doesn't mean it's impossible though. Unfortunately I don't have the silver bullet for you. Let me know if you find one, I find this stuff very interesting. That said, here's one idea I've seen for vanilla AEs. Run a bunch of data through the encoder to get a bunch of latent space samples. Then use something like Gaussian mixture modelling to estimate the density of that latent space. Then sample from that density. I've done this in the past with very simple datasets and it works, but can't speak for how good of an idea it is in general.

Again, take all this with a grain of salt, as I don't have much experience in this part of ML."
219,"I also use VAE (not from computer/information science though) and I tried to do the same thing: keeping reconstructions good while sampling for realistic new data. This can be done by betaVAE. As the other commenter said it is enough to use a hyperparameter for only the KLD part of loss with beta, which is the part responsible for reparameterize. But even if you balance the reconstruction and KLD loss, you need to compromise from both sides. Further improvement might be cost annealing. There are several strategies to do this, gradually increasing the beta in different ways -the optimum way must be found. What I ended up in my model is to keep beta very low for early epochs and let them model learn reconstructions then gradually increase the beta and then train the model in more epochs. To see how much the reconstruction and KLD losses are and how many epochs are necessary for them to increase/decrease, you may train and evaluate model at beta=0 and beta=1 first. Then you can try to find best steps and beta values.

- Again, please consider I’m from a different field and trying to use these methods in my research."
220,"Calculations won’t kill your card. Long sustained workloads may (e.g. training for days) though. Either way the lifespan will be reduced and the memory module may start throwing up random errors during games and you’ll see weird artifacts. 

If you’re training toy models for a short while it should be just fine."
221,"That example doesn't seem to be using the gpu. You need to send your computation to mps using .to(""mps"").

Try out pytorch-lightning if you want to have it taken care of automatically. I suggest going through some basic tutorials from their website.

Also, Pytorch doesn't utilise the neural engine as well as tensorflow does, yet. So, you're better off creating a prototype on mac and have it run on Google Colab or cloud VMs for gpu/tpu."
222,"CUDA is very mature framework and its actively improved by Nvidia, compared to that apple silicon is relatively new. Code for all tensor related ops must be optimised according to the hardware to achieve maximum utilisation.

On top of that, Nvidia GPUs have a lot more CUDA cores than apple's GPU cores (which are definitely high performance); It's numbers game can't say for sure if it can beat Nvidia once all the optimisations are made. Cuda at the same time is also being improved. Nvidia GPUs are currently the only choice for fast training of NNs."
223,"You are missing the scheduler state dict (But so is the PyTorch instruction)

Als your checkpoint saving is basically a copy of the standard PyTorch instructions. No hate, just honest: I dont see any value added by your article which isnt provided already by standard documentation."
224,You will not be able to write a complete NLP architecture from scratch. Coding them would take much much time if you are a beginner for no reason when some already exist and the big one like gpt requires thousands of gpus and insane amount of data to be trained.
225,"Pick a Question answering model from Hugging face and fine tune on it. There are plenty of examples on how to do that. 

If you want to make something from scratch there are a lot of ways to do so with RNN LSTMs. Your imagination is the limit."
226,Best documentation ever: [https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register\_buffer#torch.nn.Module.register\_buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_buffer#torch.nn.Module.register_buffer)
227,"I put together a short tutorial for facetorch that describes how to run it reliably using minimum amount of steps and gives examples of data it can produce.  
[https://medium.com/@gaj.../facetorch-user-guide-a0e9fd2a5552](https://medium.com/@gaj.../facetorch-user-guide-a0e9fd2a5552)"
228,"Depends what you're doing with the model. Getting classification percentages for your own use? Yes, you'll need to add the softmax. Plugging the output into the native implementation of Cross Entropy Loss? No, that's taken care of in the loss function."
229,"In general, it makes sense to do a small grid search first to get a feel of how the parameters affect each other for the usecase.

If you want or have to tune parameters sequentially, tune the most ""drastic"" first, since they will affect the other parameters the most. I.e. architecture first, then learning rate and batch size, then everything else"
230,"CUDA Runtime : in GPU driver, on the GPU. Use terminal command “nvidia-smi” to check. If you are on a non-windows PC a different driver might be enabled by default. 

CUDA Toolkit : Software on system. Allows programs to use GPU for calculations. Terminal command “nvcc  -version” to check. 

Nvidia has great install instructions. Read carefully and installation is easy.

Dont forget to add CUDA to your environment variable if Windows / or .bashrc if Linux. 


Add CuDNN if you wana get fancy, but not necessary"
231,"I recently started learning PyTorch. One of my main issues with PyTorch coming from a keras background was it's lack of premade everyday functionalities like plotting the loss, loading a dataset, etc.

Snippet extensions for editors like VS-Code solve a big part of this problem but the problem with extensions is that they don't necessarily work with cloud based tools like Google Colab and Kaggle.

So I made a website for it. currently it hosts about 35 useful snippets that I have gathered along the way but hopefully it will grow in the future. currently the snippets include these topics: PyTorch, Numpy, Pandas, Matplotlib, OpenCV, ...

[https://ghd-snippets.vercel.app/](https://ghd-snippets.vercel.app/)

https://github.com/Gholamrezadar/ghd-snippets-next

any feedbacks and shares are appreciated."
232,"You can inspect the computation graph after running the backward pass on a sample input by tracing the `_grad_fn`  attribute.

It can be a bit a pain, so you might wanna use [torchviz](https://github.com/szagoruyko/pytorchviz) to get a visualization."
233,"This article explains what you’re looking for I believe. If you’re just interested in the code, there is a Python notebook at the end. I know it focuses on multi layer bi directional lstms, but extracting the outputs you want should be a similar process 

https://towardsdatascience.com/understanding-the-outputs-of-multi-layer-bi-directional-lstms-13ad99a80dd3"
234,"If I understand you correctly, then it is not possible. Here the issue: [https://github.com/pytorch/pytorch/issues/31423#issuecomment-1150840537](https://github.com/pytorch/pytorch/issues/31423#issuecomment-1150840537)."
235,">hidden cell states

I am a bit confused about this, could you explain this?

Could it possibly be done by using [LSTMCell looping through](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)?"
236,"I did this with a pose estimation problem for robotics.

Using the torch-vision models (with pre-trained) rates worked great for my use-case. Yes they were built for classification but it seems like “transfer-learning” them to a slightly different use-case works well"
237,"You’re probably looking for the concept known as Weight Sharing, which lets the backpropagation be aware of the balance between nodes that are structurally similar. Perhaps you could duplicate the repeated layer, and share the weights between similar subgraph positions?"
238,"We still have openings for the study if anyone is interested. To set up a qualifying interview please email or PM me.  focusgroup@evansdata.com

 Compensation: Study participants receive $250 (PayPal) for a one-hour interview."
239,"Hi Vlad, are you trying do implement an embedding layer ? If yes you can use ‘’’nn.Embedding’’
In PyTorch line 78 become 
‘’’
self._particle_type_embedding = nn.Embedding(self._num_particle_types, particle_type_embedding_size)
‘’’"
240,"Without any additional information noone can help you...
Which chatbot? What kind of voice modulation? What have you done / tried before? Maybe a link to GitHub? What kind of model are we talking about? Etc, etc...."
241,"When you don’t transfer the model back to CPU, and somehow a reference to the model remains in the current scope, the memory is not recycled and thus keeps on using the GPU memory. This can easily happen, for instance, when the reference to the model is stored as a property of ‘self’ of some long living object."
242,"i believe this function sometime is confusing haha, pls [re-read the doc again](https://pytorch.org/docs/stable/generated/torch.argsort.html):

>Returns the **indices that sort a tensor** along a given dimension in ascending order by value.

it returns the **~~sorted indices of your original tensor values~~** indices of the element in sorted order. So, 2, 0, 1 here are the original indices for tensors 3, 5, and 9 in your declared **a**

5 -> index 0

9 -> index 1

3 -> index 2

asc order: 2, 0,  1"
243,"You didn’t include your data pipeline, do you have one? I would also recommend just choosing an architecture for a first pass making a model and add flexibility later once you know it works, the code is difficult to read and debug this way"
244,"The code looks mostly standard, but net.zero\_grad() might be the issue. You are zeroing the gradients of all the parameters in your model, where typically you want just optimizer.zero\_grad(). Here is a link explaining why we need this in more detail:   


[https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)  


Typically you have the following 3 lines, often immediately following the other   
1) optimizer.zero\_grad()  
2) loss.backward()  
3) optimizer.step() 

Also consider using a PyTorch dataloader class where you can shuffle the data, unless you have implemented shuffling outside of the train function."
245,"I agree with the other poster about the lack of shuffling (big issue IMO) and the questionable zero_grad.

What you said about the biggest difference in the models is also pretty interesting though — what are the actual model definitions?  Because there can be pretty large differences between tensorflow and PyTorch classes, even if they superficially sound like they do the same things.

Also, if the tensorflow output isn’t one hot encoded, what is it?  And what other minor changes are there?  I’d have started by exactly replicating the model first before looking to framework issues."
246,"Not really, the 'best' you could do is the CPU version. Because you are using docker, use multistage builds and you can have 1 layer that installs pytorch, then one that installs yours and other packages, then install your model, you will have an intermediate layer that is unchanged and this does not need to be re-downloaded."
247,"Fwiw there are open source frameworks / projects that aim to be exactly what you're asking for (tiny PyTorch).

For example; [tinygrad](https://github.com/geohot/tinygrad)
> ""This may not be the best deep learning framework, but it is a deep learning framework."" -geohot

Have you tried exporting your PT model to ONNX and wrapping it in an inference runtime like [ONNX runtime](https://onnxruntime.ai/) or importing it to TF Lite (reduced ops)? This would remove PyTorch as a dependency in the inference stage."
248,"You can check out my session to reduce pytorch image by a half (49%), all the code is available in github and blog [link](https://haicheviet.com/machine-learning-inference-on-industry-standard/#dockerize-application)"
249,If I understand what you mean by “shuffling inside the batch” (change the order of samples inside the batch) — it has no meaning as the loss is averaged over the entire batch. As for keeping the same samples in the batch — I think the way to do that it to implement a new sampler class (see [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler)).
250,"Some combination of a custom Sampler, BatchSampler, and/or collate\_fn will let you do that. Keep in mind that unless you're doing something weird the order within a batch doesn't matter since losses are typically summed or averaged over the batch, which is a permutation invariant operation."
251,"You could create your own batches within get_item then set batch_size 1 shuffle True on dataloader

Within your training loop you can shuffle your self-created batch. You might also have to consider gradient accumulation"
252,"    conda create -n oldtorch -c pytorch pytorch==0.4.1

The above ought to do it. The pytorch channel for Anaconda has the legacy version you’re looking for.

https://anaconda.org/pytorch/pytorch/files?version=0.4.1"
253,"Second line is correct. `argsort` returns the sorted indexes of all the values, so, since it is `[5,9,3]` the right ordered indexes are `[2,0,1]`\->`[3,5,9]`. Keep in mind that index starts at 0, so they are 0,1,2 and that, by default, `argsort` has dim=-1 which, in your example is dim=1."
254,"They don't, it's awful.

Researchers are under the pressure of publish or perish. They go for the quick-and-dirty and have usually no concern for code readability or maintainability or even using version control.

They are not engineers, they don't care for user experience, stability, intuitiveness. They are the only ones reading their own code.

It's also one of the reasons so many papers come without code, they are ashamed.

Otherwise I recommend you have a look at this guy repos, it's simple but effective without architecture astronauting: https://github.com/lucidrains

_A deep learning software engineer who is disgusted by most academic code_"
255,"I agree with the other comments saying academic code is often messy. However, during my PhD, I’ve put extra time into ensuring my code is readable and extensible. Yes I’ve have to sink time into refactoring that could have been spent on other stuff, but in long I believe it has saved me time as I now have a stable code base that I can refer back to.

In terms of structure, I have an OOP style back end that contains common models, datasets, trainers, evaluation methods etc. for my work. Then for a particular project or when I want to experiment, I import that project as a git submodule and I have all my existing code ready to go, so I can focus on the experimental stuff without worrying about the boilerplate stuff. I also keep scripts separate from code, so if anyone asks ‘how do I train a model’, I can direct them to the script that has all the arguments etc. and they can follow the code/comments from there. 

A couple of things to note about this. 1) It was mostly necessary as I deal with a niche area of machine learning, so there wasn’t really support for a lot of the stuff I needed, hence I had to write data loaders and stuff from (mostly) scratch (still using PyTorch though!) 2) It gets a little annoying when I do several projects in parallel that all make changes to the back end. I end up using several different versions of my back end (managed with branches) but down the line I know I’ll eventually have to bring them together and might find myself in merge conflict hell.

Hope that’s helpful!"
256,"I agree with many of the comments here; researchers often don't properly structure their code. But it's actually very important to do so for your own good:

* It is very easy to make a mistake in a complex mess, leading you to make the wrong conclusion, which leads you to unproductive paths for your research
* By properly structuring your code (using OOP and/or functional programming principles) you can try out different approaches in a controllable way
* Properly structured code is easier to read and used by others, increasing the potential for successful application of your research.

So how do you get to structured code? Of course you need to learn how to apply OOP and functional programming principles. But there is one other principle I like to share: **be the janitor of your own code**, meaning, constantly (i.e. almost daily) refactor your code to improve its structure, and to document and clean it.

Good luck!"
257,"Oh definitely following this! Any open source code links would be so useful. 

I usually use kedro to run pipelines that I build using OOP and design patterns this way i can ensure everything is a component that can be maintainable and extendable and more importantly - repeatable (through tests)

I usually draft initial ideas in jupyter notebooks and then translate them to OOP when im comfortable with the process and the class design."
258,"So far I haven't witnessed implementations that deviate markedly from:

\- utils

\- preprocessing

\- models

\- training

\- test

Believe the need for good structure arises from managing the million training runs one might do, and to preserve them for reference/eval. What is separated as modular code depends on what inputs one is changing.

And then there can be data versioning.

Within these modules, it kinda depends on the data characteristics in addition to what one is implementing. At base, there is the required Model class. Standard training objects."
259,"I don’t I just start out with a colab and it turns into a huge clusterfuck of repeated code, unordered cells, duplicated imports and overall coding nightmare.  Once in a while it actually becomes a thing/useful and I end up rewriting everything in a GitHub repo with proper file structure, commit messages and docs."
260,"> Also, by image, I actually wanted to make use of local prices within 500m, and put that on a 2D place.  

I do not think such data can/should be treated as an image.

Are you sure you do not simply want to use the local prices to do a [2d interpolation](https://docs.scipy.org/doc/scipy-1.7.1/reference/reference/generated/scipy.interpolate.interp2d.html) of the price and use it as a feature ?"
261,"Haven't tried to run any code, but quick clarification:

Your model is:

    self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=2)
    self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=2)
    self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)

and you say ""Applying the zero-padding to each convolutional layer ensures that the output have the same dimensions as inputs for each layer."" Shouldn't the padding for each of the 3 layers be 4, 0, and 2, respectively ((kernel_size - 1)/2)?"
262,"Pardon my ignorance. Is this similar to Topaz Sharpen AI like? I've tried deblurring (trying to sharpen) using NAFNet RED, but I guess deblurring differs from sharpening goals, my objective is to refine artistic drawing lines, maybe you could drive me the right path."
263,"I’ve been working through Deep Learning with PyTorch from Manning publishing. It’s been pretty nice and they go into the details of the tensor implementation without getting bogged down. There are also some nice example projects to work through.


https://www.manning.com/books/deep-learning-with-pytorch"
264,"Quick google search brought up [this](https://community.amd.com/t5/knowledge-base/amd-rocm-hardware-and-software-support-document/ta-p/489937):

>Supported GPUs

>Because the ROCm Platform has a focus on particular computational domains, we offer official support for a selection of AMD GPUs that are designed to offer good performance and price in these domains.

>Note: The integrated GPUs of Ryzen are not officially supported targets for ROCm.

>ROCm officially supports AMD GPUs that use the following chips:

>* GFX9 GPUs

>- ""Vega 10"" chips, such as on the AMD Radeon RX Vega 64 and Radeon Instinct MI25

>- ""Vega 7nm"" chips, such as on the Radeon Instinct MI50, Radeon Instinct MI60 or AMD Radeon VII, Radeon Pro VII

>* CDNA GPUs

>- MI100 chips such as on the AMD Instinct™ MI100


>ROCm is a collection of software ranging from drivers and runtimes to libraries and developer tools.
Some of this software may work with more GPUs than the ""officially supported"" list above, though AMD does not make any official claims of support for these devices on the ROCm software platform.

>The following list of GPUs is enabled in the ROCm software, though full support is not guaranteed:

>* GFX8 GPUs
* ""Polaris 11"" chips, such as on the AMD Radeon RX 570 and Radeon Pro WX 4100
* ""Polaris 12"" chips, such as on the AMD Radeon RX 550 and Radeon RX 540
* GFX7 GPUs
* ""Hawaii"" chips, such as the AMD Radeon R9 390X and FirePro W9100

>As described in the next section, GFX8 GPUs require PCI Express 3.0 (PCIe 3.0) with support for PCIe atomics. This requires both CPU and motherboard support. GFX9 GPUs require PCIe 3.0 with support for PCIe atomics by default, but they can operate in most cases without this capability.

>The integrated GPUs in AMD APUs are not officially supported targets for ROCm.
As described [below](#limited-support), ""Carrizo"", ""Bristol Ridge"", and ""Raven Ridge"" APUs are enabled in our upstream drivers and the ROCm OpenCL runtime.
However, they are not enabled in the HIP runtime, and may not work due to motherboard or OEM hardware limitations.
As such, they are not yet officially supported targets for ROCm.

>For a more detailed list of hardware support, please see [the following documentation](https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units)."
265,"I would recommend:

* PyTorch Tutorials found on the official [website](https://pytorch.org/tutorials/recipes/recipes_index.html)
* The book Deep Learning with PyTorch by Daniel Voigt Godoy

edit: typo"
266,"I am an AI research engineer and started to write advanced Pytorch practices in Medium. Feel free to check:

[https://medium.com/@yozkose3/advanced-pytorch-practices-functions-1-302296300b9e](https://medium.com/@yozkose3/advanced-pytorch-practices-functions-1-302296300b9e)"
267,">First, how can I rewrite inputs = tf.keras.layers.Input((sz, sz, c), name=""ginput"") in Torch? There is no nn.Input() function out there.

The inputs are just passed into the model. You can subclass the `torch.nn.Module` class like Keras subclassing API, and the `forward` method will handle your input, for example:

    class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)
    def forward(self, x): # this one
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

&#x200B;

>When I try to do the same in Pytorch like this:

    gd6 = nn.BatchNorm2d(gd6)
    # Input is sz4 x sz4 x 2f
    gd6 = torch.cat([gd6, ge2])

>I am told that Torch expects a tuple or list of tensors for \`gd6\`. How can I adjust it as such?

I might need more info on the error, but the code looks right. Maybe `gd6` and/or `ge2` is not a `torch.Tensor`?

&#x200B;

>Finally, in TensorFlow, there is a 2d cropping operation like this:  
>  
>`outputs = tf.keras.layers.Cropping2D(pad, name=""gcrop"")(gd8)`  
>  
>I couldn't figure out exactly how to do this in Pytorch.

I think you could use `torchvision.transforms.functional.crop` for this, or you could just use indexing like `cropped_image = image[:, :, :128, :128]` for an image of shape `(batch_size, channels, height, width)`."
268,"Use a common backbone and add 2 heads. Get the loss for each head individually, add (or average) them and backprop from the combined loss. You can also backprop from each loss individually, it does not matter"
269,"Self-promotion, but I made a framework exactly for this use case :) [https://github.com/hristo-vrigazov/dnn.cool](https://github.com/hristo-vrigazov/dnn.cool)

Have not worked on it for a few months though :)"
270,"[TorchData](https://github.com/pytorch/data) is the next gen data loading library for pytorch pipelines. They have a [CSV parser](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.CSVDictParser.html?highlight=parse_csv_as_dict) to load tabular data without the need of other libraries. It's designed to be part of a pipeline, so instead of loading the whole CSV into memory at once (which sometimes is not possible), that's a generator class that reads the CSV by line and can be chained together with a multitude of other transformations, including grouping, combining, and splitting operations - [https://pytorch.org/data/main/torchdata.datapipes.iter.html](https://pytorch.org/data/main/torchdata.datapipes.iter.html).

There are some examples to help you start using it. There's not a stable release as of today, but PyTorch is [moving towards](https://pytorch.org/blog/pytorch-1.11-released/#introducing-torchdata) these pipelines, so it's a good tool to have in your toolset.

You might also be interested in [DataFrameMaker](https://pytorch.org/data/main/generated/torchdata.datapipes.iter.DataFrameMaker.html#torchdata.datapipes.iter.DataFrameMaker)."
271,"It will be really fast, hdf5 is meant to do this. The hdf5 library will only read the portion of the file you request. 

You will need to be careful about where you open the file, because file handlers cannot be pickled, which happens when the dataloader creates multiple workers. Basically, if you open the HDF file in the dataset. __init__ you need to have something like this in your dataset (sorry if bad formatting,  on mobile)
 
```
def __getstate__(self):
    """"""
    Remove the un-pickleable file handler from the dataset instance so the dataset can be used in multiprocessing 
    """"""
    self.hdf5_file.close() # close the file 
    self.hdf5_file = None # make sure no references to the file in the dataset
    return self.__dict__ # return this objects internal dict to be pickled

def __getitem__(self, idx):
    if self.hdf5_file is None:
        ... # code to open the file
    item = ... # load item for hdf5 file
    return item
```"
272,"Somehow I replaced my post with the images (reddit newby)... 

I was asking whether anyone had similar issues with more complex models using the new apple silicon support in pytorch (using 'mps' as device)? The images are produced using the SwinIR model (https://github.com/JingyunLiang/SwinIR) . The first image shows the correct result run on the cpu and the second image the result using the GPU on my M1 max."
273,"Using a new environment might do it for now, but the real issue is that the default package solver for Conda is garbage and is being replaced by `libmamba` \- you can follow these instructions to upgrade it: [https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community)"
274,"Are you sure that you do not have some sort of "".to(device)""  call somewhere on your data that doesnt release? If you do it in a loop its fine as long as the reference is dropped between each loop iteration, otherwise you will end up with this OOM error (i.e. if you do .to(device) and then save this in a list)"
275,"Note; just because you *can* use ROCm, doesn't mean that you should. Nvidia cards are market dominant for a reason. I really wish Jen Hsun Huang would let go of our collective balls, given that Nvidia as a company are dicks, but they've got the tensor cores and the CUDA libraries, and AMD just hasn't matched them yet. So if you're in the market for a GPU, go Nvidia."
276,"new vision related models' weights are updated:

[https://pytorch.org/vision/main/models.html#table-of-all-available-classification-weights](https://pytorch.org/vision/main/models.html#table-of-all-available-classification-weights) 

new torchvision v0.13 is supported with multi-weights API:

[https://pytorch.org/blog/pytorch-1.12-new-library-releases/](https://pytorch.org/blog/pytorch-1.12-new-library-releases/)"
277,"to update parameter in place you must use the signature p.copy_(<---your update goes here--->) otherwise your update wont actually take place in the working memory of nn.Module subclass. 

You don't need to detach any parameter tensor. You can check with parameter name like you are doing, or else you can disable grad for it before initializing your optimizer and check if parameter requires grad before update."
278,"RNN's are nonlinear dynamical systems and attention to their properties is important.

There's a significant research literature on RNN's of various forms and the consequences of poor parameter choices.

I'm getting into this and I think the standard coded initializations in common packages are poor for RNN's as opposed to feed forward networks where the now standard init choices are pretty good.  I think this also contributes to the reputation as being difficult to train.  Once upon a time, deep feedforward nets were also hard to train but better initialization and hacks like batch-norm alleviate their dynamical systems problems, manifested as exploding or vanishing gradient---or both at the same time, expansion in some directions and contraction in others. 

In particular I think the square hidden to hidden weight matrices in RNNs should be initialized close to identity (singular values/eigenvalues of the overall hidden to hidden transformation are critical to stability/trainability, ideally close to 1 so there is little chaos or dissipation).  There are some papers on guaranteeing entirely unitary hidden-to-hidden dynamics (unitary like in electromagnetism/quantum mechanics) which will preserve long time-scale behavior but those require custom weight matrix implementations, i.e. compositions of elementary rotations and not a straight forward square set of FP multiply. 

Then the biases of forget gates should be fairly high because these correspond to the timescale of typical number of iterations/past values which should inform the current state. Often you want long term memory (more than a few states).

Initialize biases to b \~ log(U(1,T\_max-1)) according to a paper with T\_max the longest timescale of interest (in # of steps).   Assuming the the sense is such that the gate keeping the old hidden state is sigmoid(b), so that sigmoid(b) is close to 1, which retains the old hidden state with that coefficient and mixes in only a little bit of the new information.

You have to think much more like physical dynamical systems with RNN's, because that's what they are.

To some degree the non RNN transformer architectures bypass these issues by directly physically retaining past states and referencing them like direct memory connections."
279,"[https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD#advanced](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)

provided in their Github, under Advanced section"
280,"You can use pytorch for anything you want really, the power of pytorch comes with the gradient calculations which are a major part of any gradient based optimisation problems such as within deep learning. In my opinion you use pytorch to leverage effective/fast implementation/extension of custom models. Pytorch can be used to make systems intelligent hence a part of AI in general."
281,"I just made another post regarding larger than memory dataset in PyTorch because I am thinking of moving away from TF for the same reason for production models. It genuinely does seem like there is a gaping hole of the size of `tf.data.Dataset` in the PyTorch ecosystem. I absolutely detest the `tfrecord` format, but it does allow a standardization in data loading which PyTorch seems to lack right now, especially when dealing with TBs of data on distributed file systems. I have put together ""hacky"" dataloaders using things like `fsspec`/`gcsfs` but none of them do the job well enough for my liking.  

I have hopes that PyTorch developers will fill it up given their renewed focus on making PyTorch more productionizable."
282,"See current plans for pytorch and data : https://github.com/pytorch/data

(See question ""Q: How is multiprocessing handled with DataPipes?"" in FAQ)

As such, I think there's no plan to make a multithreaded equivalent to td.data.Dataset, it will still use multiprocessing with the DataLoader object.

I guess pytorch traded flexibility with ease of creating a new operator to computional efficieny, given the bottleneck is very rarely in the data loading pipeline."
283,What's the reason you need to avoid python multiprocessing/pickling? Are you possibly running into an x-y problem? I have run into issues with pickling and memory use in datasets  that have a goofy hdf5 setup and huge python lists but have always been able to work around it without too much trouble.
284,"How about loading the data as spark data frame on a spark cluster? Then perform operations such as count(), distinct(), filter() etc using spark. You could also perform any transformations such as converting a column to a , say BERT embedding, in a distributed fashion using spark. Once basic operations are performed use Uber’s petastorm package to load batches for training directly from the spark dataframe without having to gather all data in memory

https://blog.munhou.com/2020/01/16/Data-Pipeline-From-Pyspark-to-Pytorch/"
285,"Three reasons:

1. Inertia/Community.  At the time I started my PhD TF was what was mostly being used by others in my department so that's where I started, and I just stuck with it.  

2.  In my opinion TF is still superior for deployment across multiple devices.

3. Tensorflow probability makes it extremely easy to do drop-in replacements on existing architectures to convert them to probabilistic models.  I've looked into Pyro and other offerings in pytorch and I haven't found anything with the same ease of use as TFP where you can simply and directly replace layers in an existing model specification with probabilistic layers - if someone knows of something like that I'd love to hear about it.

IMO the primary weakness of tensorflow is just that the switch from 1.0 to 2.0 and also Keras made finding resources a pain because results for different versions that are incompatible are all floating in the same search space on google and programming forums / stack overflow.  So whenever you're trying to figure out how to do something you have to wade through a lot of different answers that are for the wrong version and won't work."
286,"I typically decide between TF and Torch depending on what third party or specialty frameworks I want to use. For instance, TF Probabilities has some nice capabilities that I haven't seen replicated in Torch yet, and Nvidia Modulus is Torch-only (as far as I know right now). So I kind of flip flop between those two depending on the application.

At my job, we also have some home-baked utilities for TF that come in handy now and then, so depending on the project for work, I will use TF if I think I might need those.

Edit: TF has Keras, which is a nice API for simplifying lots of things. Torch now has Torch Lightning, which I recommend familiarizing yourself with for some easier magic if you want to use Torch."
287,"I have had these sudden drops as well, I assumed it was the learning rate being too high or the optimizer’s momentum.

I don’t know the exact terminology but I think it has to do with finding a a good minimum during gradient descent then pushing through that and dropping into nothing.

Would love to hear from an expert if that’s correct."
288,"\*the parameters of your model may differ 

while training the model. the shape of training data is (no.of images,height of image,row of image,no. of color channel ) for mnist digits it usually looks like (6000,28,28,1), meaning the input to your first layer is of shape (None,28,28,1)

so when making a prediction on single image , you need to have a same input for your model's first layer , hence your input for your model.predict should be of similar shape as your training data 

so for predicting one image your input shape should be (1,28,28,1), but when you read a single image what you get is (28,28,1) 

to convert it to appropriate shape you need to add 1 more dimension to your 0th axis , basically `np.expand_dims(img,axis=0)`"
289,"I'm guessing it only takes a 3 dimensional input, which includes the batch size. You're feeding it 4 dimensions, (None,100,2,2048), where None is the batch size. 

Likely you'll have to have two input layers one for each camera view and where each input layer has shape (100,2048), not including batch size"
290,"A sun dial that only tells you when it’s noon once a day can be accurate. An atomic clock that tells time in milliseconds but is 17.67 hours off is precise. 

A computer vision model that always recognizes a cat from a dog (but gets the wrong breed) is accurate. One that always confuses red foxes for Maine Coon cats is precise. Optimizing a model involves improving both accuracy and precision."
291,"Here are some short definitions:

Accurate things are free of bias.
Precise things are free of uncertainty.

If I measure with the cm numbers on a ruler, but I report in inches, I'll have a precise (low uncertainty) measurement, because I'm certain if I measure again it'll be the same value. But, it won't be accurate, since it's biased to be 2.5~ times too big, every time.

If I measure with a stick broken off at 1 meter, I'll be imprecise, since I'm not certain what part of the break to measure from. But, I'll be accurate, since my measurements won't be biased away from truth."
292,"As you said, a tensor is a generalization of things like vectors and matrices. A vector is a 1D tensor (e.g., v = [0, 1, 2]) and a matrix is a 2D tensor (e.g., m = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]). These correspond to your notion of vectors and matrices in CS as well.

For these examples, you have v[1] = 1, m[1] = [3, 4, 5], and m[1][1] = 4, etc. That is, an element of this vector is a scalar, while the element of a matrix is a vector (and an element of that vector is a scalar).

An array, in your given programming language, is just a vector with a specific data structure (math doesn't care how you implement it). If you have an array of equally sized arrays, that's a matrix. Your programming language may not know what a matrix is (or a vector, for that matter), but you can construct them that way."
293,"The architecture of the models that you initialize before loading the weights should be identical, check that. And also, if you were working on the other script, different weights might be loaded beforehand so maybe try restarting the kernel.

While working, even if you do not save the model weights after training (as for example a h5 file), they are stored as variables in the background."
294,"You can look into K-means clustering, an unsupervised learning approach. My knowledge on unsupervised learning is limited. However, K-means clustering is a popular approach used for recommender systems and sentiment analysis."
295,"In Rasa/Kore.ai ,the option is there for bot to learn on its own from conversation. Bot has to learn the intent and store in knowledge base  and Train the model once in day (my theory based on my work). Check some medium articles on Rasa/kore.ai/Dialogflow to get the context."
296,"Aside from what the others have said, I would try to use your last sentence to classify those thresholds of similarity yourself.

So: do we want color, shape, object, etc. What types of similarity can exist across these photos? If you can define these “features”, you could potentially extract them from the images separately and create “new” features. Then do the k-neighbor or clustering or PCA, etc. 

Another idea would be to try to create an auto-encoder / decoder type model. So the image itself is the “label” and with enough of these the auto enconder section will find features on its own to categorize things."
297,"Lots of solutions here, but I would go with:
Load images and image names as a key:value pair in one dictionary, then the csv as image name and label as key:value pair in another.

Then you can use the image name to create the X and Y (image data and labels in this case) lists that you can feed into your CNN"
298,"The weights in your NN are probably going to extreme values, or zero. This would cause any inputted values to either be 'zeroed' out or 'clipped', resulting in functionally the same value propagating through your NN. You could probably see this in the weights if you tried to visualize it. The BatchNormalization is ""forcing"" the outputs at each layer into a more reasonable range that's preventing this ""overflow"".

I see you're trying to layer LSTMs after each other. In my, admittedly limited, experience I ran into similar issues with that kind of setup if my input data was really noisy or not overwhelmingly correlated with my ys. The internal weights in the LSTMs would just head off to infinity to me. You could try BatchNormalization just between those layers and see if that works. If so, you know that one or both of those layers has some element(s) that are running away on you and you can try different methods or maybe activation functions. Or maybe just leave them in.

As for why you wouldn't want to use it everywhere, it adds computation complexity, which adds time. It also make the network larger, and potentially more difficult to test and work with. It can also hide bad design."
299,"As pointed out by @Curld, the pipeline.config files are one and the same from what you see inside the configs folder as well as what you see in the saved_models.....I had used both and after sometime using only the config file that comes with the saved_model..... Whenever there is an updation by way of model training, when Google themselves train/evaluate and save the model, it saves the pipeline.config along with it..... If you had done training/evaluation and export it as saved_model, you will see the pipeline.config file in the exported-model folder....."
300,"Are you trying to train with TFLite?

&#x200B;

>My overall goal is to get an image via http from the local network, and give the detection back as a JSON, so that my smarthome system can run the script, and process the result.

[https://github.com/snowzach/doods2](https://github.com/snowzach/doods2)"
301,"You can easily switch from a binary to a multi class classification. Then it should work. The target should be then an array in this format: [[0,1],[..]] and you will have two output neurons instead of one with a softmax function"
302,"Is your OS windows? If yours is Linux, then Visual Studio is not required....... If you have multiple installations of Visual Studio, then uninstall the older versions and try installing Cuda again...."
303,"Code for https://arxiv.org/abs/2002.11434 found: https://github.com/kiraving/SegGradCAM

[Paper link](https://arxiv.org/abs/2002.11434) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:2002.11434/code)



--

To opt out from receiving code links, DM me"
304,"At a guess, the activation function for your final layer is restricted to output between 0 and 1?

If you're doing regression to actual values, not 1 hot categories, your activation function can just be linear (that is, none)."
305,"When you compile the model, you aren't passing the loss function.  Specifically, you have a single loss named GetLoss, and the loss function is then a lambda that just returns y_pred.  Of course there are no gradients, the loss does not depend on any part of the network, given that it is simply y_pred."
306,"Buy a used 3080 from the sleue of ETH miners and use that for a year or so while you learn. Even that is probably overkill for what you will be doing.

The big limiter for you is going to be memory. More memory in a gpu means bigger models and batch sizes. Moreso than the number of cores (as long as you are on a 30 series)."
307,"You seem to be coding and picking up fast and kudos to you... I could see you are on Windows.....Can you not change to linux (distros like Ubuntu) will be much better......I have never logged into a linux in my lifetime, but I swapped to Ubuntu a year before and it's been a fantastic learning and progress for me......If you have a home lab, you can try dual boot between Windows and Ubuntu....."
308,"Best to have the exact versions of you are using binaries, if you want to use a different cuda version you might need to build from source.

That being said , on Ubuntu I just have multiple cuda versions installed, so I use whichever is required"
309,"If you are using Conda for your TF2.2, then try installing the cuda 10.1 and cuDNN from Conda channel......Once you have installed the Cuda 10.1 and cuDNN from Conda channels, check from within the TF2.2 Conda environment, if you are able to import tensorflow with DSO library pointing to cuda 10.1....."
310,"If you can post a link (text file) of all the messages starting from your CLI, to the first 100 epochs during training, then we can see how it's progressing..... Training TF OD API on your system's GPU involves lots of interconnected dependencies and the if you can post the text file, we can make out any warnings which has to be debugged..."
311,"It is indeed possible. Jupyter notebooks are great and all for developing models but in the end you gotta deploy your model somewhere, a web app, a mobile app, or a microcontroller like Arduino.

The problem is Arduinos and other MCUs have usually very little RAM, so that's why [Tensorflow Lite](https://www.tensorflow.org/lite/microcontrollers) is a thing. As long as you can work with TF Lite limitations you can deploy your models in embedded systems easily"
312,"Hello, you should definitely look into mediapipe. It is a python package that extracts the keypoint coordinates of BOTH of your hands and your body. You can search for ""mediapipe hand keypoints"" and follow the tutorials, quite simple to setup. The data is presented as a dictionary, so you will have to extract and flatten them to an array. 

Since you have not mentioned timeseries data, your job is much easier."
313,"A friend researched for his master’s degree hand pose classification. If I recall correctly, he used a leap motion for getting features and trained a neural network for classification. Do you wanna use images or other data, like from a leap motion sensor?"
314,"Your training loss seems to be 0 but your total loss (which includes validation/testing loss) seems to be quite high, which means your model is overfitting just by looking at this figure. Resnet is a huge network and would require more data (images) to generalize your results. You are performing well on the known data but when the model sees unknown data it performs poorly because the model is too dense for the limited data you have. 

&#x200B;

Either reduce the model/network complexity or gather more images. In the case of resnet, we should be talking in the order of thousands of images."
315,"I'm not an expert on object detection but I think your activation function is dying so the CNN doesn't learn anything after x amount of epochs.
Is the output of the prediction good before the 200 epochs ? 
You can also try checking the loss graph and see how the model is performing on training and validation. 
I cannot provide much help without the code, sorry."
316,"What was the total loss after the first 100 steps? If you can share the screenshots of Tensorboard, I can visualise how the training got started and see the trend? Were you able to run testing concurrently (on another terminal)...If you are using a GPU, it will be difficult to run training and testing simultaneously...A workaround is to point the testing python files to CPU....."
317,"> student 

Chances are if you're learning, you don't need to buy a GPU. Free resources like the free tiers of Kaggle, Colab, and Paperspace will have you mostly covered, and you can get more with a small investment in cloud resources if you need it (e.g. Colab pro, or a real cloud VM like Lambda Cloud, Jarvislabs, Vast.ai, or even GCP/AWS). If you want it for gaming and this is a plus then make your decisions based on that, don't try and kid yourself or you'll make decisions that aren't optimal for you. If you really are looking at it as a DL workhorse, do the math as to whether you'll save more by having it than you would spend on cloud GPUs. Don't forget as well that the cloud services are more flexible, you can choose a GPU based on what you need at any time.

As far as the 4080 16GB vs 3090ti, nothing is definite until independent benchmarks/reviews come out, but if we take Nvidia at their word then it comes down to ""are you sure you'll never need more than 16GB for the life of this card?"". If you're sure, then the 4080 will most likely be faster (the higher clock speed alone should more than make up for the 10% reduction in core count, discounting any other architectural improvements). If you're not sure, then the 3090ti is probably a better bet, as working around VRAM limits is a huge PITA."
318,"&#x200B;

I add the loss over iterations for a simple detection task (detect masks) with Faster R-CNN with ResNet v1 640x640INFO:tensorflow:Step 5000 per-step time 0.134s

&#x200B;

INFO:tensorflow:Step 5000 per-step time 0.134s

I0416 15:41:50.554307 25636 model\_lib\_v2.py:705\] Step 5000 per-step time 0.134s

INFO:tensorflow:{'Loss/BoxClassifierLoss/classification\_loss': 0.021997754,

'Loss/BoxClassifierLoss/localization\_loss': 0.0,

'Loss/RPNLoss/localization\_loss': 0.1615103,

'Loss/RPNLoss/objectness\_loss': 0.32755154,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.5110596,

'learning\_rate': 0.038344227}

I0416 15:41:50.555308 25636 model\_lib\_v2.py:708\] {'Loss/BoxClassifierLoss/classification\_loss': 0.021997754,

'Loss/BoxClassifierLoss/localization\_loss': 0.0,

'Loss/RPNLoss/localization\_loss': 0.1615103,

'Loss/RPNLoss/objectness\_loss': 0.32755154,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.5110596,

'learning\_rate': 0.038344227}

INFO:tensorflow:Step 5100 per-step time 0.170s

I0416 15:42:07.582987 25636 model\_lib\_v2.py:705\] Step 5100 per-step time 0.170s

INFO:tensorflow:{'Loss/BoxClassifierLoss/classification\_loss': 0.0043046214,

'Loss/BoxClassifierLoss/localization\_loss': 0.0,

'Loss/RPNLoss/localization\_loss': 0.18969062,

'Loss/RPNLoss/objectness\_loss': 0.2631665,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.45716172,

'learning\_rate': 0.03823368}

I0416 15:42:07.584987 25636 model\_lib\_v2.py:708\] {'Loss/BoxClassifierLoss/classification\_loss': 0.0043046214,

'Loss/BoxClassifierLoss/localization\_loss': 0.0,

'Loss/RPNLoss/localization\_loss': 0.18969062,

'Loss/RPNLoss/objectness\_loss': 0.2631665,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.45716172,

'learning\_rate': 0.03823368}

INFO:tensorflow:Step 5200 per-step time 0.244s

I0416 15:42:32.081234 25636 model\_lib\_v2.py:705\] Step 5200 per-step time 0.244s

INFO:tensorflow:{'Loss/BoxClassifierLoss/classification\_loss': 0.07473527,

'Loss/BoxClassifierLoss/localization\_loss': 0.10821745,

'Loss/RPNLoss/localization\_loss': 0.10864566,

'Loss/RPNLoss/objectness\_loss': 0.13674596,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.42834437,

'learning\_rate': 0.038119733}

I0416 15:42:32.083235 25636 model\_lib\_v2.py:708\] {'Loss/BoxClassifierLoss/classification\_loss': 0.07473527,

'Loss/BoxClassifierLoss/localization\_loss': 0.10821745,

'Loss/RPNLoss/localization\_loss': 0.10864566,

'Loss/RPNLoss/objectness\_loss': 0.13674596,

'Loss/regularization\_loss': 0.0,

'Loss/total\_loss': 0.42834437,

'learning\_rate': 0.038119733}

INFO:tensorflow:Step 5300 per-step time 0.241s

I0416 15:42:56.219134 25636 model\_lib\_v2.py:705\] Step 5300 per-step time 0.241s

INFO:tensorflow:{'Loss/BoxClassifierLoss/classification\_loss': 0.028865596,

'Loss/BoxClassifierLoss/localization\_loss': 0.0,

'Loss/RPNLoss/localization\_loss': 0.47756785,

'Loss/RPNLoss/objectness\_loss': 0.3082205,

'Loss/regulariza"
319,"There are multiple papers about model compression. [AdaDeep is a good reference because they implement the work from multiple researchers](https://arxiv.org/pdf/2006.04432.pdf). The techniques can be divided in pruning, low-rank factorization, knowledge distillation and quantization. In pruning, weights or neurons that are deemed unnecessary are removed. If the magnitude of a weight is below a threshold, the weight is removed as it does not have much impact in the accuracy. You can also prune weights by adding constraints or regularizers. If you make your weights sparse, you can replace the dense matrix by a sparse matrix."
320,"I was also faced with this dilemma 3 years ago when I started learning DL. These GPUs are not sufficient for advanced models and are not required for beginners. As a student, you wouldn't necessarily require more performance than offered by Kaggle or colab. If you are in a university, check if they have an HPC facility. During the courses or projects which require you to develop advanced models, you will probably get access to the HPC. But still 3090ti with 24GB VRAM would be ""more useful""."
321,"This is why `tf.keras.layers.Layer.call` has an optional `training` parameter. Pass this through properly, and use it to decide whether you're training or inferring. An example of where this is done is the `BatchNormalization` layer.

https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call
https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization#call-arguments"
322,The error is coming from the fact that you are calling the \`\_.adapt(...)\` method which returns \`None\`. If you can edit your question and describe what you are trying to do I can help you refactor your code.
323,"It is a dumb question but I need to ask, you are changing 'test2.jpg' for every run, right? Since your prediction code line 9 is assigned to that image, it is not a surprise you get same output everytime."
324,"It looks like you're using binary crossentropy. It's been a minute since I've tinkered with it, but the expected output should be a single value between 0 and 1.

https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy

> Use this cross-entropy loss for binary (0 or 1) classification applications. The loss function requires the following inputs:

> * y_true (true label): This is either 0 or 1.
> * y_pred (predicted value): This is the model's prediction, i.e, **a single floating-point value** which either represents a logit, (i.e, value in [-inf, inf] when from_logits=True) or a probability (i.e, value in [0., 1.] when from_logits=False).

In this case, the prediction value represents a confidence spectrum between the two classes, where 0 aligns with complete confidence in one class and 1 aligns with complete confidence in the opposing class."
325,"I believe you can see [Forcing eager execution in tensorflow 2.1.0](https://stackoverflow.com/questions/61843285/forcing-eager-execution-in-tensorflow-2-1-0).  

You can pass `run_eagerly = True` when you compile the model with `model.compile(...)`. However you will probably find the model is going to run much more slowly in eager execution. I would recommend converting your python code into something that can be traced. Recommend you look at [Better performance with tf.function](https://www.tensorflow.org/guide/function)"
326,"The proper solution is to write your code using tensorflow, not numpy or similar. Use tensorflow's random number generator, and so on.

If you can not, create your non-tensorflow data in a generator, and wrap it with `tf.data.Dataset.from_generator`, and pass that to the network.

Both these solutions should give you a better architecture and better performance than forcing eager mode, which is an other option."
327,"Your base model already is functioning as the input layer, you do not need to specify the input shape again as that layer is not the input layer

Edit: It should be like this

base_model = tf.keras.applications.MobileNetV2(input_shape=(96,96,3),
include_top=False,
weights='imagenet')

model = tf.keras.Sequential()

model.add(base_model)

model.add(layers.Conv2D(60, kernel_size=(5, 5), strides=(1, 1),
activation='relu'))

model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(layers.Conv2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))

model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(layers.Flatten())

model.add(layers.Dense(500, activation='relu'))

model.add(layers.Dense(10, activation='softmax'))`"
328,"Input size = 96 x 96 x 3 means output size will be 3 x 3 x 3 since mobilenet v2 downsizes 32 times, if you try to apply convolution to 3 x 3 x 3 you get 1 x 1 x  3 and it is not equal to the second input shape you specified. Use images with more resolution or try other networks."
329,"It seems that you are new to machine learning, so I'll try to keep my explanation simple. 

Autoencoders learn an efficient encoding of your input data. They basically do dimensionality reduction to reduce the footprint of the data, which is useful for many tasks in ML. You can use autoencoders to reduce the dimensionality of the data and then restore the data from its reduced form. 

For autoencoders, you pass x_train both as input data and target labels because you want to check how well the model can reconstruct the data. The low-dimension encoding of your data is only good if you can bring the data back to its original state. 

For example, if you feed an image (input data) into an autoencoder during training, the model will compress and then decompress it. The model will then compare the decompressed output with the original image (target label) to check how well it has reconstructed the image from its compressed state. This is why you call model.fit(x_train, x_train)"
330,"I’d recommend checking the [documentation](https://keras.io/api/models/model_training_apis/#evaluate-method) to see what the default return value is 

I believe that link goes right to it, but if not just scroll down to the section about the evaluate() function 

Documentation is your best friend :)

You can find that page by typing something like “tensorflow model.evaluate() documentation” into google"
331,"It's probably going to take about a week to build TensorFlow with your hardware.

    apt update
    apt install -y --no-install-recommends wget build-essential openjdk-11-jdk zip unzip
    mkdir /bazel && cd /bazel
    wget https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel-4.2.1-dist.zip -O bazel.zip
    unzip bazel.zip
    EXTRA_BAZEL_ARGS=""--tool_java_runtime_version=local_jdk"" bash ./compile.sh
    mv /bazel/output/bazel /usr/bin/

[Dockerfile](https://github.com/snowzach/doods2/blob/58a9481138ce790a591fa89a142231be254785fe/docker/Dockerfile.base)"
332,"You can combine datasets by [Sampling from Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets), [choosing from Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#choose_from_datasets) or [concatenating datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate)"
333,"They provide big part of the code and whole data, you just build the model, compile it with proper loss etc. and you train the model. First question is the easiest question and one by one difficulty increasing. I solved first 4 question and had problem with 5th question and I failed at my first trial. Best way to prepare for this exam is taking Laurence Moroney's course called deep learning specialization. And last tip, work harder for time series because it will be the hardest question."
334,"Change RMSProp initializer from string to float as shown below.

>model.compile(loss='binary\_crossentropy',optimizer=tf.keras.optimizers.RMSprop(learning\_rate='0.001'),metrics=\['acc'\])

to 

>model.compile(loss='binary\_crossentropy',optimizer=tf.keras.optimizers.RMSprop(learning\_rate=0.001),metrics=\['acc'\])

It worked on my box."
335,"You don't need `@tf.function` in call. That's already handled by keras itself.

Chances are your training data / loss is not good, so the network falls into some local minimum. Think about whether your problem can be solved by your network, and how expensive it is.

Also, try adding tensorboard logging for the gradients and the loss."
336,I have only worked with multiple devices connected physically to the server. [Check this function to connect to remote devices.](https://www.tensorflow.org/api_docs/python/tf/config/experimental_connect_to_host).
337,"You probably could get it to work and use one or both GPUs, but the reason the gpu speeds stuff up is you can process lots of data on it and transferring the data to the gpu and back is likely to be much slower than running the job on the cpu with direct access to the data."
338,This is pedantic but Ring doesn’t use object detection it uses motion detection. It simply has a zone and if movement is seen within it it pings. I have a couple of their devices and neither of them actually identify what the object is.
339,"Most likely they use something like PIR sensor or something to capture motion not object/person detection. However, training a model can take a long time, while predictions can be faster. Is the model running slow making predictions or training?"
340," You forgot about the hardware. The doorbell certainly has dedicated neural chips (TPU). A neural chip has a lower frequency speed clock than a regular CPU (e.g., Intel Xeon). But since a standard CPU is designed to do any tasks per clock and a Neural chip only Vectors-related calculations, the CPU will only be slower. I think this doorbell uses FreeRTOS + an AI framework dedicated for onboard devices and C++ models… maybe TensorFlow lite. This does not prevent some AI computations are made in the cloud. May I suggest you buy the Coral TPU, it's currently cheap as fuck."
341,"You would need to use a layer that produces a fixed length size for any input shape. For example, data with shape 100, 150 and 112 would generate 50, 50 and 50. You could use a custom layer that divides each instance in 50 parts and calculates the max value of each part. Region of Interest layer does that for images. You can start with some convolutions or LSTM and then use the custom layer."
342,"So OP you want to initialize weights and bias value of a convolutional layer to some specific values? Did i understand you correctly? If yes then keras allows you to do this like [here](https://keras.io/api/layers/initializers/). Also if you have additional layers in the network and you want to train the network.  You can always freeze the convolutional layer during training so that their weight will not be changed and training/learning will happen on the remaining part of the network. 

Sorry if I misunderstood your question and this is not the solution you are looking for."
343,"You can use the lower level `tf.nn` functions instead of keras:
https://www.tensorflow.org/api_docs/python/tf/nn/conv2d

Or you can try to make the layer non-trainable and just assign the kernel values you want to it. (I'd go for the former)."
344,"The features are what you get as output from your `Conv2D` layers, and what you pass to the next `Conv2D` layer, pooling layer or what ever else you have. If you write your custom layer by inheriting from `tf.keras.layers.Layer`, or via functional style, you have control over what you do with these."
345,"You can create a model that has multiple outputs. One output would be the model’s prediction and the other outputs would be the layers you wanna check. [Check how functional models are created](https://www.tensorflow.org/guide/keras/functional). What you need to do is create a variable per each layer output that you wanna see and when you create the model, you use [feature_map1, …, feature_map_n, model_output] instead of only using model_output. That way when you generate a prediction, you will get the feature maps and the prediction."
346,"You need to give the GPU more work to get to 100% utilization. To do that, stack your input images into a Tensor4D which should increase the batch size. It will not increase memory usage. TF takes all the memory by default."
347,"Looks like an off-by-one error: i assume 1 is batch size, 3 is number of channels and 208x208 is image size? If so, somewhere (maybe in the step right before where error occurs) there is a row or column of pixels lost, it might be somewhere in the model, particularly convolutional layers, either in base or converted model"
348,"Sure, but not for the faint of heart. You would need to find a way to replace the operating system on the device. It could probably run some version of Linux. If you take it apart or delve into it, you first need to find what kind of board or computer it is using and then be careful of the memory your model might need and adjust accordingly. It would be easier to make some kind of raspberry pie thing and put it in a waterproof enclosure."
349,"Are you sure you are actually using both GPUs?

Use the ""with tf.device"" statement when you create the model, that's when the variables of the model will be initialized on your GPU, don't do it on model.fit

You can check your System monitoring tools to see if both GPUs are maxing out on ram"
350,"Use the `CUDA_VISIBLE_DEVICES` env var to limit a running tensorflow instance to a certain subset of physical GPUs.

If you want to run multiple models on the same GPU, you should do it from within the same instance of tensorflow. If you for some reason can't or don't want to, disable memory growth:
https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth"
351,"Introduction to machine learning by Andrew NG.

A machine learning course/resource does not have to be specifically about Tensorflow to make someone improve on Tensorflow. 

In my case, grasping the most fundemental aspects of machine learning helped me understand how gigantic libraries like TV and Pyt work, both regularly and in the background. 

Everyone (arguably) can use the Sequential() api and stack layers and get a decent outcome of a supposedly decent dataset. 

But not everyone can dive deep and debug a training run where test accuracy is way higher than training accuracy (sort of an unusual case. usually it is the other way around.)

If you are asking specific to the Tensorflow library, there was a book about scikit learn and keras that had a reptile on the cover page, couldnt remember the name of it but it was great."
352,"You can freeze weights by using layer.trainable=False. You could divide the weights into three layers and then concatenate the result before applying softmax. When training, you would only access the layer you don't want to update and set trainable = False."
353,"you can check this poster and the resources mentioned in it, must be the same approach the French used and it answers all your questions: [https://www.researchgate.net/publication/355127133\_Implementation\_of\_a\_Deep\_Learning\_model\_capable\_of\_detecting\_objects\_from\_satellite\_images](https://www.researchgate.net/publication/355127133_Implementation_of_a_Deep_Learning_model_capable_of_detecting_objects_from_satellite_images)"
354,"The loss function, learning rate and optimizer are hyperparameters. In your case, the MSE favors a smaller learning rate while the MAE prefers a larger one. Adam uses momentum which may be the reason that the MSE converges using 1e-2 as the learning rate. In order to find optimal hyperparameters for your model, you could do a grid search over possible learning rates."
355,"I think you are misunderstanding, it will be helpful to understand the general shape of your data,

If you have n images, there should be n labels for those images, so you're X data would be an np array of shape (n, width, height, 3) (3 for RGB) and your Y data is just (n,) since it's just a one dimensional array of 0s or 1s (in the case of binary classification

My guess it that you actually don't have n images and n labels, but some incorrect amount and your data processing has a mistake somewhere. Maybe it's from how you were splitting your data into train, test, val?

I'd need more info to diagnose the problem any more, but let me know if that was helpful"
356,"I solved it, I was following a tutorial and the guy pu this line: `X = np.array(X).reshape(-1, 224, 224)` and I just blindly followed along. It made my pictures go wierd so i deleted it and it solved the problem."
357,"For me, what I found easiest: I used python and used regular tflite.

One that was working, I grabbed a tflite file from the edgetpu_compiler, and used the tflite call to inject the coral delegate. That way, it's literally only one extra line of code from the tflite example.

I have code that does that, here; https://github.com/chunky/sb3_to_coral/blob/master/run_tflite.py

Only branch in the code that executes, is, if edgetpu is in the filename, a different delegate is injected"
358,"I think the libaries are installed by default on your dev board. That's why you can just run the commands.

This is the code I use and modify. I have a Coral USB. 

https://github.com/google-coral/examples-camera"
359,"We didn't have this thread here for long time, but this actually looks like homework?

Protip: You probably will not succeed at your university if you can't even put any effort into trying to get other people do your homework."
360,"Easy:

```
From random import randint

stockmarket_up = randint(0,1)
if stockmarket_up == 1:
    print(""Stock market will be up today"")
else:
    print(""Stockmarket will be down today"")

```

I guarantee my algorithm will work better than anything you build and run live."
361,"Debugging the execution of a model only works when running eagerly, code that's been autographed is no longer python and can't be debugged easily. Keras will by default run all call functions with autograph. If you want to debug a keras model, pass `run_eagerly=True` to compile:
https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile (this will likely be slower though, so only good for debugging)"
362,"Have you tried using a batch size of 1? How about smaller images? I would bet on memory issues

Edit: Seeing no batch size definition in your code, I assume it is a memory issue.Start with 1, increase gradually if you can"
363,"Check style transfer documentation in Tensorflow. It covers the part where it find a similarity score between the images before transferring  style from one image to another.

You can also create your own recommender using tensorflow recommender."
364,"What I've found out so far: It's not the environment I'm using, because Jupyter dies also with the exact code and also on a similar code from yt which should work.

I have 16gb ram, i7 7th gen processor and gtx 1050ti, therefore I don't think that my notebook is the cause."
365,"Have you used the tensorflow profiler? It has some good visualizations, and gives access to a lot of helpful profiling data for optimization. With keras, using it is as easy as enabling profiling in the TensorBoard callback. I use it all the time to find bottlenecks and to guide architectural decisions."
366,"Are you performing any kind of cross validation while [model.fit](https://model.fit)()'ing? Could be because it is almost ten times higher than the regular loss after 9 epochs.

Are you using a pretrained model that somehow requires that kind of loss to be calculated? Your topic is pretty interesting to say the least, different from the ordinary stuff we see around here. Maybe it somehow requires that loss metric along with the regular RMSE."
367,"I have Tensorflow 2.9.0 installed with the latest version of the Metal plug-in, which allows Tensorflow to communicate with the M-family GPUs. You just need to be very careful while installing the environments, but it works with full compatibility (and even with GPU support)"
368,"Check your PATH and PYTHONPATH environment variables. You're importing it from someplace that comes earlier than your pip install directory. You should be able to look at `tf.__file__` to see the location.

This is why people like conda envs. You don't want packages installed globally if you need multiple versions."
369,"It is really simple with few lines of code. Just one comment in case you use custom layers. A couple of days I tried to save and load a model that had multiple custom layers. I got a bunch of weird errors that didn’t make sense. The reason was that I forgot to implement the get_config() for some of them. For example, I had a custom convolutional layer and when I loaded the model I got an error that the shapes for a matmul in fc1 didn’t match, which didn’t made sense because I trained the model without problems. Took me a while to figure out that I had missed to implement a get_config."
370,"Running on GPU or CPU?

What configuration are you using to limit memory growth? For some configurations it will just try to take all the memory, which will fail if the device is the default graphics device used by the OS. Usually this will happen before the model is even loaded."
371,"This looks like an .ipynb file right? On the file browser of the jupyter notebook, check the size of this particular .ipynb file. If you do not clear the outputs and save the file, the variables are stored and overwritten on the .ipynb file and the size goes to megabytes range. 

I had it happen to me where I had a an .ipynb file of size 120 megabytes due to not clearing all outputs and saving regularly and when I try to execute the code the RAM was climbing up until google chrome exploded."
372,"I assume you’re using local nvidia gpu, in that case Go to nvidia-smi, and kill all kernels from terminal and run again. Always kill kernel when you move from one jupyter notebook to another.. this is the common problem I faced many times."
373,"Thanks for sharing! Yeah interesting stuff with the trend of some projects having a higher proportion of contributors experiencing slower wait times. I wondered if it might be due to the pandemic's trend of more people working on side projects while staying at home etc. and as a result, having higher numbers of PRs to get through with limited maintainer capacity.

But when I actually [looked at this data](https://imgur.com/gallery/gqxybCr), that wasn't really the case. Tensorflow had a particularly spiky year in 2021, but they seemed to be legitimate PRs, not bots or anything like that. Interestingly, Tensorflow seems to be [decreasing in its number of unique PR authors](https://imgur.com/Dv6QzTk) (i.e. contributors)."
374,"Look up affine transforms, homography, and piecewise affine transforms. Points that work well (can be found accurately and are in the same plane) for these direct transforms are the only points you should train it with."
375,"Reinforcement learning. Basically, you create a neural network that takes as inputs the current state of the game as well as what move you are about to make. As an output, or label while you are training it, you get the “score” of the game. After a while of training, given the current state of the game the network can produce what it considers the optimal move to have a better score at the next game state. This is obviously an oversimplification but if you look up reinforcement learning you can find examples."
376,"What’s your access level and what’s your OS. You could chmod on MacOS and just give it full permissions, but that’s a lazy solution and giving full permissions usually isn’t a great idea. Can you show how you are accessing the folders? Are you using blobs or the os default library?"
377,"Tfrecords for storing
TFdata for the pipeline
Regular model or quantized model (serve it via a cloud service later)… there isn’t much difference between “industrial grade” and normal

It is complicated. This is why people pay consulting companies millions of dollars."
378,"I would use the complete dataset. If you only use one stock per training iteration, it only contains just a few thousand datapoints at best, which is really not enough. Concentrate on using homogeneous data ( do not mix SNP500 and penny stocks for example) and use everything you have at once. Do not use loops as it will be much slower. Your route is basically a slower and worse version of the .fit() function. Also, it is normal to hava a higher mse as 1,) you trained on different datasets, 2,) mse is compounding and averaging throughout the fit so it will always start high. The question is whether it is decreased at the end"
379,"I think you should avoid using Image data generator, it's not super flexible in letting you set up different problems and it's deprecated anyways.

I would load your df as a tensorflow dataset with

    ds= tf.data.Dataset.from_tensor_slices(df)

Or if you prefer, separate your data out into two data frames for input and output, then convert to tensorflow datasets, map your categories to numbers or one-hot encodings, and then you can use the zip tensorflow dataset method to combine.

https://www.tensorflow.org/api_docs/python/tf/data/Dataset

This page will be of great help to you"
